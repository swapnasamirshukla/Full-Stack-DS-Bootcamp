{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. Question: Explain the architecture of BERT.**\n",
    "\n",
    "Answer: BERT (Bidirectional Encoder Representations from Transformers)\n",
    "is a transformer-based model architecture for natural language\n",
    "understanding tasks. It consists of an encoder stack with multiple\n",
    "transformer layers. BERT employs a bidirectional approach, meaning it\n",
    "processes the entire input text in both directions (left-to-right and\n",
    "right-to-left), capturing contextual information effectively. During\n",
    "pretraining, BERT learns contextualized word representations by\n",
    "predicting masked words in a sentence and predicting the relationship\n",
    "between two sentences. These pretrained representations are then\n",
    "fine-tuned for specific downstream tasks.\n",
    "\n",
    "**2. Question: Explain Masked Language Modeling (MLM).**\n",
    "\n",
    "Answer: Masked Language Modeling (MLM) is a pretraining task used in\n",
    "models like BERT. In MLM, a certain percentage of words in a sentence\n",
    "are randomly replaced with a \\[MASK\\] token, and the model is tasked\n",
    "with predicting the original words based on the context provided by the\n",
    "surrounding words. This encourages the model to learn bidirectional\n",
    "context and produce meaningful representations for each word.\n",
    "\n",
    "**3. Question: Explain Next Sentence Prediction (NSP).**\n",
    "\n",
    "Answer: Next Sentence Prediction (NSP) is another pretraining task used\n",
    "in BERT. NSP aims to teach the model to understand relationships between\n",
    "sentences. Pairs of sentences are presented to the model, and it learns\n",
    "to predict whether the second sentence follows the first one in the\n",
    "original text. This task helps BERT capture the contextual relationships\n",
    "between sentences, enabling it to perform tasks involving document-level\n",
    "understanding.\n",
    "\n",
    "**4. Question: What is Matthews evaluation?**\n",
    "\n",
    "Answer: \"Matthews evaluation\" might be a typo or a misnomer. If you're\n",
    "referring to the Matthews Correlation Coefficient (MCC), please see the\n",
    "next question.\n",
    "\n",
    "**5. Question: What is Matthews Correlation Coefficient (MCC)?**\n",
    "\n",
    "Answer: The Matthews Correlation Coefficient (MCC) is a measure used for\n",
    "assessing the quality of binary classification models, especially when\n",
    "dealing with imbalanced datasets. It takes into account true positives,\n",
    "true negatives, false positives, and false negatives to provide a\n",
    "balanced assessment of a model's performance. The MCC ranges from -1\n",
    "(worst) to +1 (best), where a higher value indicates better performance.\n",
    "\n",
    "**6. Question: Explain Semantic Role Labeling.**\n",
    "\n",
    "Answer: Semantic Role Labeling (SRL) is a natural language processing\n",
    "task that involves identifying the roles that various words or phrases\n",
    "play in a sentence's predicate-argument structure. In simpler terms, SRL\n",
    "aims to understand the relationships between words in a sentence and the\n",
    "actions or events described by the sentence. It assigns labels to words\n",
    "indicating their roles as agents, patients, instruments, and other\n",
    "semantic roles.\n",
    "\n",
    "**7. Question: Why does Fine-tuning a BERT model take less time than\n",
    "pretraining?**\n",
    "\n",
    "Answer: Fine-tuning a BERT model takes less time than pretraining\n",
    "because the pretrained BERT model has already learned general language\n",
    "features and context during the pretraining phase. Fine-tuning involves\n",
    "training the model on a specific downstream task using task-specific\n",
    "labeled data. During fine-tuning, the model's parameters are adjusted to\n",
    "adapt its learned representations to the task at hand. Since the model\n",
    "has already captured meaningful contextual information in its pretrained\n",
    "layers, fine-tuning requires fewer iterations to specialize its\n",
    "knowledge for the target task.\n",
    "\n",
    "**8. Question: What is Recognizing Textual Entailment (RTE)?**\n",
    "\n",
    "Answer: Recognizing Textual Entailment (RTE) is a natural language\n",
    "processing task that involves determining whether a given piece of text\n",
    "(the \"hypothesis\") can be logically inferred from another piece of text\n",
    "(the \"premise\"). The task essentially checks whether the hypothesis can\n",
    "be considered true based on the information present in the premise. It's\n",
    "often used to evaluate the reasoning and understanding capabilities of\n",
    "models on tasks like textual entailment, paraphrase detection, and\n",
    "question answering.\n",
    "\n",
    "**9. Question: Explain the decoder stack of GPT models.**\n",
    "\n",
    "Answer: The decoder stack of GPT (Generative Pre-trained Transformer)\n",
    "models is a stack of multiple transformer decoder layers. In each layer,\n",
    "the model takes as input the output from the previous layer and\n",
    "processes it using self-attention mechanisms and feed-forward neural\n",
    "networks. The decoder stack helps the GPT model generate coherent and\n",
    "contextually appropriate text. During generation, the model processes\n",
    "the input sequence step by step, attending to previous tokens to\n",
    "generate the next token in a sequence, making it suitable for\n",
    "autoregressive language generation tasks."
   ]
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {}
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. One-Hot Encoding:**\n",
    "\n",
    "One-Hot Encoding is a technique used to convert categorical data into a\n",
    "numerical format that can be used by machine learning algorithms. Each\n",
    "category is represented as a binary vector where only one element is\n",
    "\"hot\" (1) and the others are \"cold\" (0). It's commonly used for\n",
    "converting features like gender, colors, or other non-ordinal categories\n",
    "into a format suitable for mathematical computations.\n",
    "\n",
    "**2. Bag of Words (BoW):**\n",
    "\n",
    "Bag of Words is a text representation technique used in Natural Language\n",
    "Processing (NLP). It represents a text as a collection of individual\n",
    "words, disregarding grammar and word order. Each word is treated as a\n",
    "separate feature, and the frequency of each word is counted. BoW is used\n",
    "for tasks like text classification and sentiment analysis.\n",
    "\n",
    "**3. Bag of N-Grams:**\n",
    "\n",
    "Bag of N-Grams is an extension of the Bag of Words technique. Instead of\n",
    "considering single words as features, it considers sequences of N\n",
    "consecutive words (N-grams). This approach captures some contextual\n",
    "information and can be useful for capturing short phrases and idiomatic\n",
    "expressions in text.\n",
    "\n",
    "**4. TF-IDF (Term Frequency-Inverse Document Frequency):**\n",
    "\n",
    "TF-IDF is a numerical representation technique used in NLP. It measures\n",
    "the importance of a word in a document relative to its frequency in a\n",
    "collection of documents. It combines two metrics: Term Frequency (TF),\n",
    "which measures the frequency of a word in a document, and Inverse\n",
    "Document Frequency (IDF), which penalizes words that appear frequently\n",
    "in the entire document collection.\n",
    "\n",
    "**5. OOV Problem (Out-of-Vocabulary Problem):**\n",
    "\n",
    "The OOV problem refers to words that are not present in the vocabulary\n",
    "of a language model or NLP system. This occurs when the model encounters\n",
    "words it hasn't seen during training. Handling OOV words is crucial to\n",
    "ensure the model's robustness and ability to generalize to new data.\n",
    "\n",
    "**6. Word Embeddings:**\n",
    "\n",
    "Word embeddings are vector representations of words in a continuous\n",
    "space where similar words have similar vector representations. They\n",
    "capture semantic relationships between words. Word embeddings are often\n",
    "learned from large amounts of text data using techniques like Word2Vec,\n",
    "GloVe, and FastText.\n",
    "\n",
    "**7. Continuous Bag of Words (CBOW):**\n",
    "\n",
    "CBOW is a word embedding technique where the goal is to predict a target\n",
    "word based on its surrounding context words. It takes a sequence of\n",
    "context words as input and tries to predict the central target word.\n",
    "CBOW is efficient for training and performs well on syntactic tasks.\n",
    "\n",
    "**8. SkipGram:**\n",
    "\n",
    "SkipGram is a word embedding technique that works in the opposite way of\n",
    "CBOW. It takes a target word as input and tries to predict the\n",
    "surrounding context words. SkipGram is useful for capturing semantic\n",
    "relationships between words and performs well on semantic tasks.\n",
    "\n",
    "**9. GloVe Embeddings (Global Vectors for Word Representation):**\n",
    "\n",
    "GloVe is a word embedding technique that combines elements of global and\n",
    "local context information. It leverages global statistics of word\n",
    "co-occurrence to learn word representations. GloVe embeddings are known\n",
    "for capturing both syntactic and semantic relationships between words\n",
    "and are pre-trained on large corpora."
   ]
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {}
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a867394d",
   "metadata": {},
   "source": [
    "**1. What are Vanilla autoencoders?**\n",
    "Vanilla autoencoders are a type of neural network architecture used for unsupervised learning and dimensionality reduction. They consist of an encoder and a decoder. The encoder transforms the input data into a lower-dimensional representation, while the decoder reconstructs the original input data from the lower-dimensional representation. The goal of autoencoders is to learn efficient data representations that capture important features and minimize reconstruction error.\n",
    "\n",
    "**2. What are Sparse autoencoders?**\n",
    "Sparse autoencoders are a variation of autoencoders that encourage the learned representations to be sparse, meaning that only a few neurons in the hidden layer are active at a time. This encourages the autoencoder to learn more meaningful and selective features, which can be useful for feature extraction and data compression.\n",
    "\n",
    "**3. What are Denoising autoencoders?**\n",
    "Denoising autoencoders are designed to handle noisy input data. During training, noisy versions of the input data are fed to the autoencoder, and the goal is to reconstruct the clean, original input data. This encourages the autoencoder to learn robust representations that are less affected by noise.\n",
    "\n",
    "**4. What are Convolutional autoencoders?**\n",
    "Convolutional autoencoders are autoencoders that use convolutional layers in their encoder and decoder architecture. They are commonly used for image-related tasks, where the encoder captures spatial hierarchies of features and the decoder generates reconstructions. They are particularly effective for tasks like image denoising, inpainting, and feature extraction.\n",
    "\n",
    "**5. What are Stacked autoencoders?**\n",
    "Stacked autoencoders, also known as deep autoencoders, consist of multiple layers of encoders and decoders stacked on top of each other. Each layer learns a new representation of the data. Stacked autoencoders can learn hierarchical features and are used to build deep representations for various tasks, including pretraining deep neural networks.\n",
    "\n",
    "**6. Explain how to generate sentences using LSTM autoencoders:**\n",
    "To generate sentences using LSTM autoencoders, you train the autoencoder on a dataset of sentences. The encoder processes the input sentences, and the hidden state of the encoder's last LSTM layer is used as the initial state of the decoder's LSTM layers. During generation, you input a start token to the decoder and iteratively sample words from the output distribution of the decoder at each time step. The generated words are fed back into the decoder for the next time step until an end token is generated or a maximum length is reached.\n",
    "\n",
    "**7. Explain Extractive summarization:**\n",
    "Extractive summarization is a text summarization technique where important sentences or phrases are selected from the original text to form a concise summary. The selected sentences are usually the ones that contain key information or represent the main ideas of the text. Extractive summarization doesn't involve generating new sentences; instead, it selects and rearranges existing sentences.\n",
    "\n",
    "**8. Explain Abstractive summarization:**\n",
    "Abstractive summarization is a more advanced text summarization technique where the summary is generated by paraphrasing and rephrasing the original text. It involves understanding the context of the text and generating new sentences that capture the essence of the content. Abstractive summarization often requires natural language generation techniques and can be more challenging but potentially leads to more human-like summaries.\n",
    "\n",
    "**9. Explain Beam search:**\n",
    "Beam search is a technique used in sequence generation tasks, such as machine translation or text generation, to find the most likely sequence of words given a model. It maintains a set of candidate sequences (the \"beam\") and expands it by considering multiple possible next words at each step. The sequences with the highest probabilities are kept, and this process continues until the sequences are complete.\n",
    "\n",
    "**10. Explain Length normalization:**\n",
    "Length normalization is a technique used in sequence generation tasks to mitigate the effect of length bias. Longer sequences tend to have lower probabilities due to the multiplication of probabilities at each step. Length normalization involves dividing the log-probability of a sequence by its length, penalizing shorter sequences and making the comparison fairer among sequences of different lengths.\n",
    "\n",
    "**11. Explain Coverage normalization:**\n",
    "Coverage normalization is used to prevent repetition in abstractive text summarization. It maintains a coverage vector that keeps track of which parts of the source text have been attended to during generation. By incorporating coverage information, the model can reduce redundancy and ensure that important information is not repeated excessively in the generated summary.\n",
    "\n",
    "**12. Explain ROUGE metric evaluation:**\n",
    "ROUGE (Recall-Oriented Understudy for Gisting Evaluation) is a metric used to evaluate the quality of machine-generated text summaries. It measures the overlap between the n-grams (sequences of n words) in the generated summary and the reference (human-generated) summary. ROUGE evaluates various aspects such as precision, recall, and F1-score of overlapping n-grams to assess the quality and coherence of the generated summaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c735c3bf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

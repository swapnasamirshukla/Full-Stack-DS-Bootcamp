{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f3752c40",
   "metadata": {},
   "source": [
    "**1. What are Sequence-to-sequence models?**\n",
    "Sequence-to-sequence (Seq2Seq) models are a type of neural network architecture designed for tasks that involve transforming one sequence into another. They consist of two main components: an encoder that processes the input sequence and converts it into a fixed-size context vector, and a decoder that generates the output sequence based on the context vector. Seq2Seq models are commonly used for machine translation, text summarization, and other sequence generation tasks.\n",
    "\n",
    "**2. What are the problems with Vanilla RNNs?**\n",
    "Vanilla (simple) RNNs suffer from vanishing gradient and exploding gradient problems during training. They struggle to capture long-range dependencies in sequences due to the diminishing or amplification of gradients as they are backpropagated through time. As a result, they have difficulty learning patterns from sequences with long time lags.\n",
    "\n",
    "**3. What is Gradient clipping?**\n",
    "Gradient clipping is a technique used during training of neural networks to prevent gradients from becoming too large during backpropagation. When gradients exceed a specified threshold, they are scaled down to ensure stable learning. This prevents the exploding gradient problem and helps in smoother convergence during training.\n",
    "\n",
    "**4. Explain Attention mechanism:**\n",
    "The attention mechanism is a technique used in sequence-to-sequence models to improve the model's ability to focus on relevant parts of the input sequence when generating each part of the output sequence. It assigns different weights to different parts of the input sequence based on their relevance to the current step of the output sequence. This helps the model effectively capture long-range dependencies and improves performance in tasks like machine translation and text generation.\n",
    "\n",
    "**5. Explain Conditional random fields (CRFs):**\n",
    "Conditional Random Fields (CRFs) are a type of graphical model used in sequence labeling tasks, such as named entity recognition and part-of-speech tagging. CRFs model the conditional probability distribution of labels given the input sequence. They capture dependencies between neighboring labels in the sequence and are capable of considering the entire input sequence when making predictions.\n",
    "\n",
    "**6. Explain self-attention:**\n",
    "Self-attention is an attention mechanism where the input sequence is processed to create query, key, and value representations from the same sequence. It allows the model to weigh the importance of different elements in the input sequence based on their relationship with other elements within the same sequence. Self-attention is a key component in Transformer models, which have revolutionized tasks like machine translation.\n",
    "\n",
    "**7. What is Bahdanau Attention?**\n",
    "Bahdanau Attention is a specific type of attention mechanism used in sequence-to-sequence models. It learns to align input sequence elements with output sequence elements by calculating attention scores based on the similarity between decoder hidden states and encoder hidden states. It addresses the information bottleneck problem in traditional Seq2Seq models by allowing the decoder to focus on different parts of the input sequence.\n",
    "\n",
    "**8. What is a Language Model?**\n",
    "A Language Model is a type of neural network that is trained to predict the probability distribution of words in a sequence based on the context of previous words. It is used to generate coherent and grammatically correct text, complete sentences, and perform tasks like machine translation, text generation, and speech recognition.\n",
    "\n",
    "**9. What is Multi-Head Attention?**\n",
    "Multi-Head Attention is an extension of self-attention used in Transformer models. It involves computing attention scores using multiple sets of learnable query, key, and value transformations. This allows the model to capture different aspects of the input sequence simultaneously and learn different types of relationships between words, enhancing its expressive power.\n",
    "\n",
    "**10. What is Bilingual Evaluation Understudy (BLEU)**\n",
    "Bilingual Evaluation Understudy (BLEU) is a metric used to evaluate the quality of machine-generated text, particularly in the context of machine translation. It measures the similarity between the generated output and the reference (human-generated) output using n-gram overlap. BLEU is commonly used to compare different machine translation systems and assess their performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18973288",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

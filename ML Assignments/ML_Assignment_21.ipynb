{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a2cb79e5",
   "metadata": {},
   "source": [
    "**1. What is the estimated depth of a Decision Tree trained (unrestricted) on a one million instance\n",
    "training set?**\n",
    "\n",
    "The estimated depth of a Decision Tree trained on a one million instance training set can vary depending on various factors, including the complexity of the data and the hyperparameters set for the tree. In an unrestricted scenario (no maximum depth set), the tree could potentially grow very deep, but it may also stop growing when it achieves perfect purity (i.e., all leaf nodes are pure). In practice, it's common to see Decision Trees with depths ranging from a few levels to much deeper, depending on the complexity of the problem and the quality of the data.\n",
    "\n",
    "**2. Is the Gini impurity of a node usually lower or higher than that of its parent? Is it always\n",
    "lower/greater, or is it usually lower/greater?**\n",
    "\n",
    "The Gini impurity of a node in a Decision Tree is typically lower than that of its parent. The Gini impurity measures the impurity or uncertainty of a node, and the goal of the Decision Tree algorithm is to reduce this impurity as it splits the data into child nodes. Each split aims to make the child nodes more homogeneous, which results in a lower Gini impurity compared to the parent node. This process continues until a stopping criterion is met or the tree reaches a predefined maximum depth.\n",
    "\n",
    "**3. Explain if its a good idea to reduce max depth if a Decision Tree is overfitting the training set?**\n",
    "\n",
    "It can be a good idea to reduce the maximum depth of a Decision Tree if it is overfitting the training set. Overfitting occurs when the tree becomes too deep and captures noise in the training data, leading to poor generalization to new, unseen data. By reducing the maximum depth, you can make the tree less complex and more likely to generalize well to new data. You should monitor the model's performance on a validation set to determine the optimal depth that balances bias and variance.\n",
    "\n",
    "**4. Explain if its a good idea to try scaling the input features if a Decision Tree underfits the training\n",
    "set?**\n",
    "\n",
    "Scaling the input features is generally not necessary for Decision Trees. Decision Trees are not sensitive to the scale of the input features because they make binary decisions based on feature values and thresholds. Unlike some other algorithms like gradient boosting or support vector machines, Decision Trees do not rely on distance metrics, so feature scaling does not impact their performance.\n",
    "\n",
    "**5. How much time will it take to train another Decision Tree on a training set of 10 million instances if it takes an hour to train a Decision Tree on a training set with 1 million instances?**\n",
    "\n",
    "The time it takes to train a Decision Tree on a training set is influenced by several factors, including the algorithm used, the hardware, and software optimizations. However, as a rough estimate, training a Decision Tree on a training set of 10 million instances may take roughly 10 times longer than training it on a training set of 1 million instances. This assumes that the complexity of the tree and other parameters remain similar.\n",
    "\n",
    "**6. Will setting presort=True speed up training if your training set has 100,000 instances?**\n",
    "\n",
    "Setting `presort=True` in a Decision Tree can potentially speed up training for small datasets (e.g., 100,000 instances) but can significantly slow down training for larger datasets. When `presort=True`, the algorithm sorts the data at each node to find the best split, which can be computationally expensive for large datasets. For small datasets, the overhead of sorting might be outweighed by the benefit of faster splitting decisions, but for larger datasets, it can become impractical. In practice, it's often better to let the algorithm decide whether to use presorting based on the dataset size. Modern implementations of Decision Trees, like those in scikit-learn, usually have this optimization built in."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "501d48eb",
   "metadata": {},
   "source": [
    "**7. Follow these steps to train and fine-tune a Decision Tree for the moons dataset:**\n",
    "\n",
    "a. To build a moons dataset, use make moons(n samples=10000, noise=0.4).\n",
    "\n",
    "b. Divide the dataset into a training and a test collection with train test split().\n",
    "\n",
    "c. To find good hyperparameters values for a DecisionTreeClassifier, use grid search with cross-\n",
    "validation (with the GridSearchCV class). Try different values for max leaf nodes.\n",
    "\n",
    "d. Use these hyperparameters to train the model on the entire training set, and then assess its\n",
    "output on the test set. You can achieve an accuracy of 85 to 87 percent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "97a91154",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 5 candidates, totalling 25 fits\n",
      "Test set accuracy: 0.87\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import make_moons\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "X, y = make_moons(n_samples=10000, noise=0.4, random_state=42)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "param_grid = {'max_leaf_nodes': [10, 20, 30, 40, 50]}  # Try different values\n",
    "tree_clf = DecisionTreeClassifier(random_state=42)\n",
    "grid_search = GridSearchCV(tree_clf, param_grid, cv=5, verbose=1)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "best_tree_clf = grid_search.best_estimator_\n",
    "y_pred = best_tree_clf.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Test set accuracy:\", accuracy)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4549a855",
   "metadata": {},
   "source": [
    "**8. Follow these steps to grow a forest:**\n",
    "\n",
    "a. Using the same method as before, create 1,000 subsets of the training set, each containing 100 instances chosen at random. You can do this with Scikit-ShuffleSplitLearn class.\n",
    "\n",
    "b. Using the best hyperparameter values found in the previous exercise, train one Decision Tree on each subset. On the test collection, evaluate these 1,000 Decision Trees. These Decision Trees would likely perform worse than the first Decision Tree, achieving only around 80% accuracy, since they were trained on smaller sets.\n",
    "\n",
    "c. Now the magic begins. Create 1,000 Decision Tree predictions for each test set case, and keep only the most common prediction (you can do this with SciPy mode() function). Over the test collection, this method gives you majority-vote predictions.\n",
    "\n",
    "d. On the test range, evaluate these predictions: you should achieve a slightly higher accuracy than the first model (approx 0.5 to 1.5 percent higher). You've successfully learned a Random Forest classifier!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2213fa01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Test Set Accuracy: 0.872\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shuklas\\AppData\\Local\\Temp\\ipykernel_27552\\66612895.py:39: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  y_pred_majority_votes, _ = mode(y_preds, axis=0)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import make_moons\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from scipy.stats import mode\n",
    "from sklearn.base import clone\n",
    "\n",
    "# Create the moons dataset\n",
    "X, y = make_moons(n_samples=10000, noise=0.4, random_state=42)\n",
    "\n",
    "# Split the dataset into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# a. Create 1,000 subsets of the training set with 100 instances each\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "\n",
    "n_trees = 1000\n",
    "n_instances = 100\n",
    "subsets = []\n",
    "\n",
    "rs = ShuffleSplit(n_splits=n_trees, test_size=len(X_train) - n_instances, random_state=42)\n",
    "for train_index, _ in rs.split(X_train):\n",
    "    X_subset = X_train[train_index]\n",
    "    y_subset = y_train[train_index]\n",
    "    subsets.append((X_subset, y_subset))\n",
    "\n",
    "# b. Train Decision Trees on each subset\n",
    "decision_trees = []\n",
    "for X_subset, y_subset in subsets:\n",
    "    tree_clf = DecisionTreeClassifier(max_leaf_nodes=best_tree_clf.max_leaf_nodes, random_state=42)\n",
    "    tree_clf.fit(X_subset, y_subset)\n",
    "    decision_trees.append(tree_clf)\n",
    "\n",
    "# c. Make predictions and use majority vote\n",
    "y_preds = np.empty([n_trees, len(X_test)], dtype=np.uint8)\n",
    "for i, tree_clf in enumerate(decision_trees):\n",
    "    y_preds[i] = tree_clf.predict(X_test)\n",
    "\n",
    "y_pred_majority_votes, _ = mode(y_preds, axis=0)\n",
    "\n",
    "# d. Evaluate the Random Forest\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred_majority_votes.ravel())\n",
    "print(\"Random Forest Test Set Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "755d869c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

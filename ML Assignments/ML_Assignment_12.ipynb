{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "48895c89",
   "metadata": {},
   "source": [
    "**1. What is prior probability? Give an example.**  \n",
    "Prior probability refers to the initial or base probability of an event, before incorporating new evidence. It represents our knowledge or uncertainty about an event before considering new information.  \n",
    "*Example:* Suppose you are trying to diagnose a rare disease that affects 1 in every 10,000 people. The prior probability of a randomly selected person having the disease is 0.0001 or 0.01%.\n",
    "\n",
    "**2. What is posterior probability? Give an example.**  \n",
    "Posterior probability is the revised probability of an event occurring after considering new evidence or data. It's calculated using Bayes' theorem, which incorporates the prior probability and the likelihood of the new evidence given the event.  \n",
    "*Example:* Using the rare disease example, if a patient tests positive for the disease (and the test has certain known sensitivities and specificities), the posterior probability would calculate the probability of the patient truly having the disease given their positive test result.\n",
    "\n",
    "**3. What is likelihood probability? Give an example.**  \n",
    "Likelihood represents the probability of observing the given data or evidence under different scenarios or hypotheses. It is not a probability in itself but gives a measure to compare different hypotheses based on the evidence.  \n",
    "*Example:* In the disease context, the likelihood might represent the probability of getting a positive test result given that a patient actually has the disease.\n",
    "\n",
    "**4. What is Naïve Bayes classifier? Why is it named so?**  \n",
    "Naïve Bayes classifier is a probabilistic classifier based on applying Bayes' theorem with strong (naïve) independence assumptions between features. It's named \"naïve\" because it assumes that all features are independent of each other given the class label, which is a strong and often unrealistic assumption. \n",
    "\n",
    "**5. What is optimal Bayes classifier?**  \n",
    "The optimal Bayes classifier minimizes the probability of error in predictions. It's a theoretical classifier that assigns a class label based on the class that has the highest posterior probability given the input features. It represents the best performance achievable if the true probabilistic relationship between the features and the class labels is known.\n",
    "\n",
    "**6. Write any two features of Bayesian learning methods.**  \n",
    "- **Incorporation of Prior Knowledge:** Bayesian methods allow for the incorporation of prior knowledge or beliefs through the prior probability.\n",
    "- **Update with New Data:** Bayesian methods can be updated with new data, refining the posterior probabilities as more evidence becomes available.\n",
    "\n",
    "**7. Define the concept of consistent learners.**  \n",
    "Consistent learners are those that converge to the true hypothesis as the number of training examples approaches infinity. Essentially, given enough data, a consistent learner will eventually output a hypothesis with an error rate as low as the best possible hypothesis from its hypothesis space.\n",
    "\n",
    "**8. Write any two strengths of Bayes classifier.**  \n",
    "- **Handles Uncertainty:** Bayes classifiers naturally handle uncertainty by assigning probabilities to class memberships.\n",
    "- **Incorporation of Prior Knowledge:** They allow for the incorporation of prior knowledge which can be particularly useful if domain knowledge is available.\n",
    "\n",
    "**9. Write any two weaknesses of Bayes classifier.**  \n",
    "- **Assumption of Independence:** Especially in the case of Naïve Bayes, the assumption that features are independent can be a strong and unrealistic assumption.\n",
    "- **Sensitive to Irrelevant Features:** Because of the independence assumption, irrelevant features can impact the classifier's performance.\n",
    "\n",
    "**10. Explain how Naïve Bayes classifier is used for:**\n",
    "\n",
    "**1. Text classification:**  \n",
    "For text classification, the Naïve Bayes classifier calculates the probability of a document being in a particular class based on the frequencies of words present in the document. Words in the document are treated as features. The prior probabilities can be estimated based on the proportions of different classes in the training set, and the likelihoods can be estimated from the frequencies of words in documents of each class.\n",
    "\n",
    "**2. Spam filtering:**  \n",
    "In spam filtering, emails or messages are classified as 'spam' or 'not spam'. The words or phrases in the email are treated as features. A Naïve Bayes classifier can be trained on labeled examples to estimate probabilities based on word frequencies and then classify new emails based on these probabilities.\n",
    "\n",
    "**3. Market sentiment analysis:**  \n",
    "For sentiment analysis, a Naïve Bayes classifier can be trained on labeled data (e.g., positive, negative, neutral reviews or comments) to understand the likelihood of certain words appearing in each sentiment category. When analyzing a new piece of text, the classifier calculates the probabilities of it belonging to each sentiment category and classifies it in the category with the highest probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1829c2c2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

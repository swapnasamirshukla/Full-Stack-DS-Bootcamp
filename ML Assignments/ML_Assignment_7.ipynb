{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. What is the definition of a target function? In the sense of a\n",
    "real-life example, express the target function. How is a target\n",
    "function's fitness assessed?**\n",
    "\n",
    "Target Function: In machine learning, the target function is the true\n",
    "but unknown mapping between input variables (features) and the desired\n",
    "output (labels). It's the function we're trying to approximate with our\n",
    "predictive model.\n",
    "\n",
    "Example: In a medical diagnosis system, the target function could map a\n",
    "patient's symptoms and test results to the diagnosis of a disease.\n",
    "\n",
    "Assessing Fitness: The fitness of a target function is assessed by\n",
    "comparing the predictions made by the model with the actual outcomes\n",
    "(ground truth) using evaluation metrics such as accuracy, precision,\n",
    "recall, F1-score, etc.\n",
    "\n",
    "**2. What are predictive models, and how do they work? What are\n",
    "descriptive types, and how do you use them? Examples of both types of\n",
    "models should be provided. Distinguish between these two forms of\n",
    "models.**\n",
    "\n",
    "Predictive Models: Predictive models make predictions about future\n",
    "outcomes based on input data. They learn patterns and relationships in\n",
    "data to make accurate predictions. Example: Linear Regression, Random\n",
    "Forest for predicting stock prices.\n",
    "\n",
    "Descriptive Models: Descriptive models aim to summarize and understand\n",
    "patterns in data without making predictions. They are used to gain\n",
    "insights into data characteristics. Example: Clustering algorithms like\n",
    "K-Means for grouping customers based on behavior.\n",
    "\n",
    "Distinction: Predictive models predict future outcomes, while\n",
    "descriptive models provide insights into existing patterns without\n",
    "making predictions.\n",
    "\n",
    "**3. Describe the method of assessing a classification model's\n",
    "efficiency in detail. Describe the various measurement parameters.**\n",
    "\n",
    "To assess a classification model's efficiency, you can use the following\n",
    "measurement parameters:\n",
    "\n",
    "\\- Accuracy: The ratio of correct predictions to the total number of\n",
    "predictions.\n",
    "\n",
    "\\- Precision: The ratio of true positives to the sum of true positives\n",
    "and false positives. Measures the model's ability to correctly identify\n",
    "positive cases.\n",
    "\n",
    "\\- Recall (Sensitivity): The ratio of true positives to the sum of true\n",
    "positives and false negatives. Measures the model's ability to capture\n",
    "all positive cases.\n",
    "\n",
    "\\- F1-Score: The harmonic mean of precision and recall. Provides a\n",
    "balanced measure of a model's accuracy.\n",
    "\n",
    "\\- Specificity: The ratio of true negatives to the sum of true negatives\n",
    "and false positives. Measures the model's ability to correctly identify\n",
    "negative cases.\n",
    "\n",
    "**4.**\n",
    "\n",
    "**i. What is underfitting? What is the most common reason for\n",
    "underfitting?**\n",
    "\n",
    "\\- Underfitting occurs when a model is too simple to capture the\n",
    "underlying patterns in the data. It performs poorly on both training and\n",
    "new data.\n",
    "\n",
    "\\- The most common reason for underfitting is using a model that's too\n",
    "basic or has too few features to represent the data adequately.\n",
    "\n",
    "**ii. What does it mean to overfit? When is it going to happen?**\n",
    "\n",
    "\\- Overfitting occurs when a model fits the training data too closely,\n",
    "capturing noise and random fluctuations.\n",
    "\n",
    "\\- It happens when the model is too complex, has too many features, or\n",
    "when it's trained for too long.\n",
    "\n",
    "**iii. In the sense of model fitting, explain the bias-variance\n",
    "trade-off.**\n",
    "\n",
    "\\- The bias-variance trade-off refers to the balance between a model's\n",
    "ability to fit training data (low bias) and its ability to generalize to\n",
    "new data (low variance).\n",
    "\n",
    "\\- High bias (underfitting) means the model is too simple to fit the\n",
    "data. High variance (overfitting) means the model captures noise and\n",
    "doesn't generalize well.\n",
    "\n",
    "**5. Is it possible to boost the efficiency of a learning model? If so,\n",
    "please clarify how**.\n",
    "\n",
    "\\- Yes, you can boost the efficiency of a learning model by using more\n",
    "complex algorithms, increasing the amount of data, improving data\n",
    "quality, feature engineering, and tuning hyperparameters. Ensemble\n",
    "techniques like Bagging and Boosting can also enhance efficiency.\n",
    "\n",
    "**6. How would you rate an unsupervised learning model's success? What\n",
    "are the most common success indicators for an unsupervised learning\n",
    "model?**\n",
    "\n",
    "\\- The success of an unsupervised learning model is often measured by\n",
    "its ability to uncover meaningful patterns or structures in the data.\n",
    "\n",
    "\\- Common success indicators include silhouette score (clustering\n",
    "quality), cohesion and separation measures, and visual inspection of\n",
    "cluster characteristics.\n",
    "\n",
    "**7. Is it possible to use a classification model for numerical data or\n",
    "a regression model for categorical data with a classification model?\n",
    "Explain your answer.**\n",
    "\n",
    "\\- It's not recommended to use a classification model for numerical data\n",
    "or a regression model for categorical data. Each type of model is\n",
    "designed for a specific type of prediction task. Classification models\n",
    "predict categories, while regression models predict continuous values.\n",
    "\n",
    "**8. Describe the predictive modeling method for numerical values. What\n",
    "distinguishes it from categorical predictive modeling?**\n",
    "\n",
    "\\- Predictive modeling for numerical values involves techniques like\n",
    "linear regression, decision trees, and support vector regression. These\n",
    "models predict continuous numeric outcomes.\n",
    "\n",
    "\\- Categorical predictive modeling involves classification algorithms\n",
    "that predict categories or classes for data points.\n",
    "\n",
    "**9. Classification Model Performance with Given Data:**\n",
    "\n",
    "i\\. Accurate estimates – 15 cancerous, 75 benign\n",
    "\n",
    "ii\\. Wrong predictions – 3 cancerous, 7 benign\n",
    "\n",
    "\\- Error Rate: (3 + 7) / (15 + 75) = 0.1 (10%)\n",
    "\n",
    "\\- Sensitivity (Recall): 15 / (15 + 3) = 0.833 (83.3%)\n",
    "\n",
    "\\- Precision: 15 / (15 + 7) = 0.682 (68.2%)\n",
    "\n",
    "F-measure = 2 \\* (Precision \\* Recall) / (Precision + Recall) = 2 \\*\n",
    "(0.682 \\* 0.833) / (0.682 + 0.833) ≈ 0.751\n",
    "\n",
    "To calculate the Kappa value, we need to construct an observed agreement\n",
    "matrix that shows the agreement between the predicted and actual\n",
    "classes. Here's how you can calculate the Kappa value for the given\n",
    "data:\n",
    "\n",
    "Given data:\n",
    "\n",
    "\\- True Positives (TP) = 15 (cancerous)\n",
    "\n",
    "\\- False Positives (FP) = 7 (benign)\n",
    "\n",
    "\\- False Negatives (FN) = 3 (cancerous)\n",
    "\n",
    "\\- True Negatives (TN) = 75 (benign)\n",
    "\n",
    "Total agreements (A) = TP + TN = 15 + 75 = 90\n",
    "\n",
    "Total disagreements (D) = FP + FN = 7 + 3 = 10\n",
    "\n",
    "Total instances (N) = A + D = 90 + 10 = 100\n",
    "\n",
    "Probabilities:\n",
    "\n",
    "\\- Probability of random agreement (Pr) = (TP + FP) \\* (TP + FN) / N^2 +\n",
    "(TN + FP) \\* (TN + FN) / N^2\n",
    "\n",
    "\\- Probability of observed agreement (Po) = A / N\n",
    "\n",
    "Kappa value (K) = (Po - Pr) / (1 - Pr)\n",
    "\n",
    "Let's calculate step by step:\n",
    "\n",
    "Pr = ((15 + 7) \\* (15 + 3) / 100^2) + ((75 + 7) \\* (75 + 3) / 100^2) =\n",
    "0.1864\n",
    "\n",
    "Po = 90 / 100 = 0.9\n",
    "\n",
    "K = (0.9 - 0.1864) / (1 - 0.1864) ≈ 0.774\n",
    "\n",
    "So, the Kappa value for the given data is approximately 0.774.\n",
    "\n",
    "**10. Make quick notes on:**\n",
    "\n",
    "1\\. The process of holding out: Holding out refers to reserving a\n",
    "portion of the dataset as a validation or test set to assess the model's\n",
    "performance on unseen data.\n",
    "\n",
    "2\\. Cross-validation by tenfold: Dividing the dataset into 10 subsets\n",
    "and using each subset as the test set while training on the remaining\n",
    "nine subsets in rotation.\n",
    "\n",
    "3\\. Adjusting the parameters: Tuning hyperparameters to optimize a\n",
    "model's performance by finding the best combination of settings.\n",
    "\n",
    "**11. Define the following terms:**\n",
    "\n",
    "1\\. **Purity vs. Silhouette width:** Purity measures how homogenous\n",
    "clusters are (0.5 means balanced). Silhouette width quantifies the\n",
    "separation between clusters (higher is better).\n",
    "\n",
    "2\\. **Boosting vs. Bagging:** Boosting combines weak learners into a\n",
    "strong learner sequentially. Bagging uses bootstrap sampling to create\n",
    "diverse models and aggregates their predictions.\n",
    "\n",
    "3\\. **The eager learner vs. the lazy learner:** Eager learners build a\n",
    "generalized model before seeing the data (e.g., decision trees). Lazy\n",
    "learners postpone building a model until needed (e.g., k-NN)."
   ]
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {}
}

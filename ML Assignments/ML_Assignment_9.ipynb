{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 1: What is feature engineering, and how does it work? Explain\n",
    "the various aspects of feature engineering in depth.**\n",
    "\n",
    "Answer: Feature engineering is the process of selecting, transforming,\n",
    "or creating relevant features from raw data to enhance the performance\n",
    "of machine learning models. It involves several aspects:\n",
    "\n",
    "\\- Feature Selection: Choosing pertinent features. This enhances model\n",
    "simplicity, reduces overfitting, and speeds up training.\n",
    "\n",
    "\\- Feature Transformation: Applying mathematical operations like\n",
    "scaling, normalization, and log transforms. This ensures features are\n",
    "comparable and have the same scale.\n",
    "\n",
    "\\- Feature Creation: Generating new features from existing ones.\n",
    "Polynomial features, interaction terms, and domain-specific aggregations\n",
    "can be created.\n",
    "\n",
    "\\- Handling Missing Data: Addressing missing values through imputation\n",
    "or adding indicator variables for missingness.\n",
    "\n",
    "\\- Encoding Categorical Variables: Converting categorical data into\n",
    "numerical form, often via one-hot encoding or label encoding.\n",
    "\n",
    "\\- Handling Text and Time Data: Converting text into numerical\n",
    "representations like TF-IDF or word embeddings. Time data can be broken\n",
    "into day, month, year components.\n",
    "\n",
    "**Question 2: What is feature selection, and how does it work? What is\n",
    "the aim of it? What are the various methods of function selection?**\n",
    "\n",
    "Answer: Feature selection involves choosing a subset of relevant\n",
    "features from the original set to improve model performance and reduce\n",
    "complexity, with the goal of avoiding overfitting. Methods include:\n",
    "\n",
    "\\- Filter Methods: Evaluate features independently of the model using\n",
    "metrics like correlation, variance, or mutual information.\n",
    "\n",
    "\\- Wrapper Methods: Use a machine learning model as evaluator, testing\n",
    "subsets of features and selecting the best set.\n",
    "\n",
    "\\- Embedded Methods: Incorporate feature selection within model\n",
    "training, like LASSO regularization.\n",
    "\n",
    "**Question 3: Describe the function selection filter and wrapper\n",
    "approaches. State the pros and cons of each approach?**\n",
    "\n",
    "Answer:\n",
    "\n",
    "\\- Filter Approaches: Rank features independently using statistical\n",
    "metrics. Pros: Efficiency, computational ease. Cons: Might not capture\n",
    "feature interactions.\n",
    "\n",
    "\\- Wrapper Approaches: Use a model to evaluate subsets of features.\n",
    "Pros: Capture interactions, model-specific effects. Cons:\n",
    "Computationally intensive, prone to overfitting.\n",
    "\n",
    "**Question 4:**\n",
    "\n",
    "**i. Describe the overall feature selection process.**\n",
    "\n",
    "**ii. Explain the key underlying principle of feature extraction using\n",
    "an example. What are the most widely used function extraction\n",
    "algorithms?**\n",
    "\n",
    "Answer:\n",
    "\n",
    "i\\. Overall feature selection process involves:\n",
    "\n",
    "1\\. Define the problem and dataset.\n",
    "\n",
    "2\\. Generate potential features.\n",
    "\n",
    "3\\. Use filter methods to rank features.\n",
    "\n",
    "4\\. Apply wrapper methods to search for best subset.\n",
    "\n",
    "5\\. Evaluate model on a separate test dataset.\n",
    "\n",
    "ii\\. Feature extraction aims to reduce dimensionality while retaining\n",
    "relevant information. Example: Principal Component Analysis (PCA)\n",
    "transforms correlated features into orthogonal components that capture\n",
    "maximum variance. Other algorithms include Linear Discriminant Analysis\n",
    "(LDA) and Non-Negative Matrix Factorization (NMF).\n",
    "\n",
    "**Question 5: Describe the feature engineering process in the sense of a\n",
    "text categorization issue.**\n",
    "\n",
    "Answer: In text categorization:\n",
    "\n",
    "1\\. Tokenization: Split text into words or subword units.\n",
    "\n",
    "2\\. Stop Words Removal: Remove common, uninformative words.\n",
    "\n",
    "3\\. TF-IDF Transformation: Scale word frequencies by importance.\n",
    "\n",
    "4\\. N-grams: Include word sequences for context.\n",
    "\n",
    "5\\. Word Embeddings: Convert words into dense vector representations.\n",
    "\n",
    "**Question 6: What makes cosine similarity a good metric for text\n",
    "categorization? Calculate the cosine similarity between two\n",
    "document-term vectors.**\n",
    "\n",
    "Answer: Cosine similarity is effective for text categorization because\n",
    "it measures the cosine of the angle between two vectors, indicating\n",
    "their direction similarity regardless of magnitude. It's suitable for\n",
    "comparing text documents with varying lengths. Given vectors (2, 3, 2,\n",
    "0, 2, 3, 3, 0, 1) and (2, 1, 0, 0, 3, 2, 1, 3, 1), cosine similarity â‰ˆ\n",
    "0.669.\n",
    "\n",
    "**Question 7:**\n",
    "\n",
    "**i. What is the formula for calculating Hamming distance? Calculate the\n",
    "Hamming distance between 10001011 and 11001111.**\n",
    "\n",
    "**ii. Compare the Jaccard index and similarity matching coefficient for\n",
    "features (1, 1, 0, 0, 1, 0, 1, 1) and (1, 0, 0, 1, 1, 0, 0, 1).**\n",
    "\n",
    "Answer:\n",
    "\n",
    "i\\. Hamming distance formula: Count differing bits. Hamming distance\n",
    "between 10001011 and 11001111 = 2.\n",
    "\n",
    "ii\\. Jaccard Index: 2 common / 6 total = 0.333. Similarity Matching\n",
    "Coefficient: 4 matching / 8 total = 0.5.\n",
    "\n",
    "**Question 8: What is meant by \"high-dimensional data set\"? Offer\n",
    "examples and describe the challenges.**\n",
    "\n",
    "Answer: A high-dimensional data set has many features compared to\n",
    "samples. Examples: genomic data, images, text. Challenges include\n",
    "increased computation, sparsity, curse of dimensionality (sparse data\n",
    "and distances), and overfitting. Techniques like dimensionality\n",
    "reduction can address these.\n",
    "\n",
    "**Question 9: Provide quick notes on:**\n",
    "\n",
    "**1. PCA is an acronym for Personal Computer Analysis.**\n",
    "\n",
    "**2. Use of vectors**\n",
    "\n",
    "**3. Embedded technique**\n",
    "\n",
    "Answer:\n",
    "\n",
    "1\\. Incorrect. PCA stands for Principal Component Analysis, a\n",
    "dimensionality reduction method.\n",
    "\n",
    "2\\. Vectors are mathematical representations used in ML for data points\n",
    "in multi-dimensional space.\n",
    "\n",
    "3\\. Embedded technique combines feature selection with model training.\n",
    "\n",
    "**Question 10: Compare:**\n",
    "\n",
    "**1. Sequential backward exclusion vs. sequential forward selection**\n",
    "\n",
    "**2. Function selection methods: filter vs. wrapper**\n",
    "\n",
    "**3. SMC vs. Jaccard coefficient**\n",
    "\n",
    "Answer:\n",
    "\n",
    "1\\. Sequential backward exclusion removes least important features;\n",
    "forward selection adds most important.\n",
    "\n",
    "2\\. Filter ranks features based on criteria; wrapper uses a model to\n",
    "evaluate subsets.\n",
    "\n",
    "3\\. SMC (Similarity Matching Coefficient) for binary data; Jaccard\n",
    "coefficient for binary and non-binary, based on intersection/union."
   ]
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {}
}

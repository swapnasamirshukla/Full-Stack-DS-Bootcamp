{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2f64c4a6",
   "metadata": {},
   "source": [
    "**1. Recognize the differences between supervised, semi-supervised, and unsupervised learning.**  \n",
    "- **Supervised Learning**: The model is trained on a labeled dataset, meaning each example in the dataset is paired with the correct output.\n",
    "- **Semi-supervised Learning**: The model is trained using a combination of a small amount of labeled data and a large amount of unlabeled data. The idea is to use the unlabeled data to enhance the learning from the labeled data.\n",
    "- **Unsupervised Learning**: The model is trained on an unlabeled dataset, trying to learn the underlying structure of the data, such as clustering or reducing dimensionality.\n",
    "\n",
    "**2. Describe in detail any five examples of classification problems.**  \n",
    "- **Email Filtering**: Classifying emails as spam or not spam.\n",
    "- **Image Recognition**: Identifying if an image contains a cat or a dog.\n",
    "- **Loan Approval**: Determining if a loan should be approved or denied based on applicant details.\n",
    "- **Disease Diagnosis**: Predicting if a patient has a particular disease based on symptoms.\n",
    "- **Sentiment Analysis**: Classifying a text review as positive, negative, or neutral.\n",
    "\n",
    "**3. Describe each phase of the classification process in detail.**  \n",
    "- **Data Collection**: Gathering raw data relevant to the problem.\n",
    "- **Data Preprocessing**: Cleaning data, handling missing values, and converting non-numeric data into numeric form.\n",
    "- **Feature Selection/Extraction**: Choosing the most relevant features or creating new features from the existing ones.\n",
    "- **Model Selection**: Choosing the appropriate classification algorithm.\n",
    "- **Training**: Feeding the training data into the classifier to train it.\n",
    "- **Evaluation**: Testing the classifier on unseen data and evaluating its performance using metrics like accuracy, precision, recall, etc.\n",
    "- **Deployment**: Implementing the classifier in a real-world system.\n",
    "- **Monitoring and Updating**: Continuously monitoring the classifier's performance and retraining it with new data if necessary.\n",
    "\n",
    "**4. Go through the SVM model in depth using various scenarios.**  \n",
    "- **SVM (Support Vector Machine)** is a supervised machine learning algorithm that can be used for both classification and regression. The main idea behind SVM is to find a hyperplane that best separates the data into classes. In scenarios where data is not linearly separable, SVM uses a kernel trick to transform the data into a higher dimension where it becomes separable.\n",
    "\n",
    "**5. What are some of the benefits and drawbacks of SVM?**  \n",
    "- **Benefits**:\n",
    "  - Effective in high-dimensional spaces.\n",
    "  - Works well when the margin of separation is clear.\n",
    "  - Memory efficient as it uses only a subset of training points (support vectors).\n",
    "- **Drawbacks**:\n",
    "  - Not suitable for large datasets due to high training time.\n",
    "  - Doesn't perform well when the dataset has more noise, i.e., target classes are overlapping.\n",
    "  - Requires scaling of input data.\n",
    "\n",
    "**6. Go over the kNN model in depth.**  \n",
    "- **kNN (k-Nearest Neighbors)** is a simple, instance-based learning algorithm. To classify a new instance, kNN identifies 'k' training examples that are closest to the point and returns the most common output value among them. The distance is typically calculated using Euclidean distance, but other metrics like Manhattan distance can also be used.\n",
    "\n",
    "**7. Discuss the kNN algorithm's error rate and validation error.**  \n",
    "- The **error rate** of kNN is the fraction of incorrect predictions made by the model on the training data. The **validation error** is the fraction of incorrect predictions made on a separate validation dataset. As 'k' increases, the error rate typically decreases, but only to a point, after which it starts increasing due to over-smoothing.\n",
    "\n",
    "**8. For kNN, talk about how to measure the difference between the test and training results.**  \n",
    "- The difference can be measured using metrics like accuracy, precision, recall, etc. A significant difference between training and test accuracy indicates overfitting.\n",
    "\n",
    "**9. Create the kNN algorithm.**  \n",
    "- The kNN algorithm involves:\n",
    "  1. Choose the number 'k' and a distance metric.\n",
    "  2. For a new data point, compute its distance to all points in the training set.\n",
    "  3. Select the 'k' training examples with the smallest distances.\n",
    "  4. Return the most common output value among the 'k' neighbors.\n",
    "\n",
    "**10. What is a decision tree, exactly? What are the various kinds of nodes? Explain all in depth.**  \n",
    "- A **decision tree** is a flowchart-like structure where each internal node represents a feature(or attribute), each branch represents a decision rule, and each leaf node represents an outcome. The tree splits the data based on feature values, aiming to achieve pure leaf nodes (i.e., nodes with data points from a single class).\n",
    "  - **Root Node**: The topmost node, which involves splitting the data based on the feature that gives the most significant information gain.\n",
    "  - **Decision Node**: Nodes that make decisions, leading to sub-nodes.\n",
    "  - **Leaf/ Terminal Node**: Nodes that represent the final outcome or class label.\n",
    "\n",
    "**11. Describe the different ways to scan a decision tree.**  \n",
    "- Decision trees can be traversed in various ways:\n",
    "  - **Pre-order Traversal**: Visit the root, traverse the left subtree, then traverse the right subtree.\n",
    "  - **In-order Traversal**: Traverse the left subtree, visit the root, then traverse the right subtree.\n",
    "  - **Post-order Traversal**: Traverse the left subtree, traverse the right subtree, and then visit the root.\n",
    "\n",
    "**12. Describe in depth the decision tree algorithm.**  \n",
    "- The decision tree algorithm involves:\n",
    "  1. Select the best attribute using Attribute Selection Measures(ASM) like Information Gain, Gain Ratio, or Gini Index.\n",
    "  2. Make that attribute a decision node and break the dataset into smaller subsets.\n",
    "  3. Start tree building by repeating this process recursively for each child until one of the conditions matches:\n",
    "     - All the tuples belong to the same attribute value.\n",
    "     - There are no more remaining attributes.\n",
    "     - There are no more instances.\n",
    "\n",
    "**13. In a decision tree, what is inductive bias? What would you do to stop overfitting?**  \n",
    "- **Inductive Bias** in decision trees is the set of assumptions the learner uses to predict outputs for new inputs. For decision trees, the bias is a preference for trees that are closer to the root (shorter trees). \n",
    "  - To prevent overfitting:\n",
    "    - Prune the tree by removing branches that have little importance.\n",
    "    - Set a minimum limit on the number of samples in a leaf.\n",
    "    - Limit the maximum depth of the tree.\n",
    "\n",
    "**14. Explain advantages and disadvantages of using a decision tree?**  \n",
    "- **Advantages**:\n",
    "  - Easy to understand and visualize.\n",
    "  - Requires little data preprocessing.\n",
    "  - Can handle both numerical and categorical data.\n",
    "- **Disadvantages**:\n",
    "  - Prone to overfitting, especially with a large number of features.\n",
    "  - Can be unstable as small variations in data can result in a different tree.\n",
    "  - Biased trees if some classes dominate.\n",
    "\n",
    "**15. Describe in depth the problems that are suitable for decision tree learning.**  \n",
    "- Decision trees are suitable for:\n",
    "  - Problems where interpretability is essential.\n",
    "  - Non-linear problems.\n",
    "  - Classification problems with a mix of numeric and categorical features.\n",
    "  - When you want to understand the importance of different features.\n",
    "\n",
    "**16. Describe in depth the random forest model. What distinguishes a random forest?**  \n",
    "- **Random Forest** is an ensemble learning method that creates a 'forest' of decision trees. Each tree is trained on a random subset of the data and makes its own predictions. The random forest aggregates these predictions to produce a final result.\n",
    "  - What distinguishes a random forest is that it introduces randomness in both data sampling (using bootstrapping) and feature selection for each split, making it more robust and less prone to overfitting compared to a single decision tree.\n",
    "\n",
    "**17. In a random forest, talk about OOB error and variable value.**  \n",
    "- **OOB (Out-of-Bag) Error**: Since random forests use bootstrapping to sample data for each tree, about one-third of the samples are left out during the training of each tree. These left-out samples are called out-of-bag samples. The OOB error is the average error for each training sample calculated using predictions from the trees that do not contain that training sample.\n",
    "  - **Variable Importance**: In a random forest, variable importance measures the increase in prediction error when the values of a particular variable are permuted. Variables that are more important for prediction will result in a higher increase in prediction error when their values are permuted.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

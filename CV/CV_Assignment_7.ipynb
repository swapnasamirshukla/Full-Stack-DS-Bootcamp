{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eafad9a5",
   "metadata": {},
   "source": [
    "**Question 1: What is the COVARIATE SHIFT Issue, and how does it affect you?**\n",
    "\n",
    "Covariate shift refers to changes in the distribution of input data between different stages of training. It affects training because the model might be learning from data that is no longer representative of the validation/test data. This can lead to poor generalization and reduced performance on unseen data.\n",
    "\n",
    "**Question 2: What is the process of BATCH NORMALIZATION?**\n",
    "\n",
    "Batch normalization is a technique that normalizes the activations of each layer within a mini-batch during training. It involves computing the mean and variance of activations and then normalizing them. This helps stabilize training by reducing internal covariate shift and can speed up convergence.\n",
    "\n",
    "**Question 3: Using our own terms and diagrams, explain LENET ARCHITECTURE.**\n",
    "\n",
    "LeNet is an early convolutional neural network (CNN) architecture designed for handwritten digit recognition. It consists of a series of convolutional and pooling layers followed by fully connected layers. The architecture diagram typically shows the flow of data through convolutional, activation, and pooling layers, followed by flattening and fully connected layers leading to the final output.\n",
    "\n",
    "**Question 4: Using our own terms and diagrams, explain ALEXNET ARCHITECTURE.**\n",
    "\n",
    "AlexNet is a pioneering CNN architecture for image classification. It comprises multiple convolutional and pooling layers, followed by fully connected layers. Its key innovation was using ReLU activations, dropout for regularization, and data augmentation to prevent overfitting. Diagrams would depict the layers, their connectivity, and how they process the input data.\n",
    "\n",
    "**Question 5: Describe the vanishing gradient problem.**\n",
    "\n",
    "The vanishing gradient problem occurs during deep neural network training when gradients become extremely small as they are backpropagated through multiple layers. This leads to shallow layers not learning effectively as weight updates become negligible, hindering convergence and making training difficult.\n",
    "\n",
    "**Question 6: What is NORMALIZATION OF LOCAL RESPONSE?**\n",
    "\n",
    "Normalization of Local Response, also known as Local Response Normalization (LRN), is a technique used in some neural networks to enhance the response of neurons by normalizing the activity of neighboring neurons. It's used to provide a form of lateral inhibition, allowing the strongest responses to become even stronger relative to their neighbors.\n",
    "\n",
    "**Question 7: In AlexNet, what WEIGHT REGULARIZATION was used?**\n",
    "\n",
    "In AlexNet, L2 weight regularization (also known as weight decay) was used. It involves adding a penalty term to the loss function that discourages large weight values. This helps prevent overfitting by encouraging the network to use smaller weights.\n",
    "\n",
    "**Question 8: Using our own terms and diagrams, explain VGGNET ARCHITECTURE.**\n",
    "\n",
    "VGGNet is a CNN architecture known for its simplicity and effectiveness. It consists of a series of convolutional layers with small 3x3 filters, followed by pooling layers. The architecture diagram would illustrate the repeated pattern of convolutional and pooling layers, leading to fully connected layers for classification.\n",
    "\n",
    "**Question 9: Describe VGGNET CONFIGURATIONS.**\n",
    "\n",
    "VGGNet has different configurations named by the number of convolutional layers in each stack. For example, VGG16 has 16 convolutional layers, while VGG19 has 19. These layers are grouped into blocks with a pattern of increasing filter sizes and decreasing spatial dimensions.\n",
    "\n",
    "**Question 10: What regularization methods are used in VGGNET to prevent overfitting?**\n",
    "\n",
    "In VGGNet, dropout and weight decay (L2 regularization) are common regularization techniques to prevent overfitting. Dropout randomly deactivates neurons during training to reduce reliance on specific neurons, and weight decay discourages large weights by adding a penalty term to the loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87c4409e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bd0f1d5a",
   "metadata": {},
   "source": [
    "**Question 1: What is the concept of cyclical momentum?**\n",
    "\n",
    "Cyclical momentum refers to the practice of changing the momentum value during training cycles in a neural network. Momentum is a parameter that controls how much of the past gradient updates should be taken into account when updating weights. By varying momentum values cyclically during training, it's possible to improve convergence and escape local minima.\n",
    "\n",
    "**Question 2: What callback keeps track of hyperparameter values (along with other data) during training?**\n",
    "\n",
    "The `Recorder` callback in fastai keeps track of hyperparameter values, losses, and metrics during training. It records this information, allowing you to visualize and analyze the training process.\n",
    "\n",
    "**Question 3: In the color dim plot, what does one column of pixels represent?**\n",
    "\n",
    "In the color dim plot, each column of pixels represents the values of a specific color channel for a given image. The x-axis represents the spatial dimension (width), and the y-axis represents the pixel intensity in that channel.\n",
    "\n",
    "**Question 4: In color dim, what does \"poor teaching\" look like? What is the reason for this?**\n",
    "\n",
    "\"Poor teaching\" in the context of color dim refers to colors being clumped together and not well-distributed across the color space. This can happen due to biases in the training data, inadequate data augmentation, or limitations in the model's capacity to capture color variations.\n",
    "\n",
    "**Question 5: Does a batch normalization layer have any trainable parameters?**\n",
    "\n",
    "Yes, a batch normalization layer has trainable parameters. It learns two types of parameters: scale (gamma) and shift (beta) parameters. These parameters allow the network to scale and shift the normalized activations to achieve a desired distribution.\n",
    "\n",
    "**Question 6: In batch normalization during preparation, what statistics are used to normalize? What about during the validation process?**\n",
    "\n",
    "During training, batch normalization uses the mean and standard deviation of the activations within the current batch to normalize the data. During validation or inference, the statistics used are the running mean and running standard deviation, which are computed and updated during training.\n",
    "\n",
    "**Question 7: Why do batch normalization layers help models generalize better?**\n",
    "\n",
    "Batch normalization helps models generalize better by normalizing activations, making them less sensitive to changes in input distribution. This reduces internal covariate shift, which can stabilize and speed up training, allowing the model to converge faster and generalize better to new data.\n",
    "\n",
    "**Question 8: Explain the difference between MAX POOLING and AVERAGE POOLING.**\n",
    "\n",
    "Max pooling takes the maximum value from each local region of the input, reducing the dimensions while retaining dominant features. Average pooling takes the average value from each local region, which can help preserve more general information.\n",
    "\n",
    "**Question 9: What is the purpose of the POOLING LAYER?**\n",
    "\n",
    "The pooling layer reduces the spatial dimensions of the input while retaining important features. It helps reduce the model's sensitivity to translations and enhances its ability to recognize patterns irrespective of their position in the input.\n",
    "\n",
    "**Question 10: Why do we end up with Completely CONNECTED LAYERS?**\n",
    "\n",
    "Fully connected layers (also called dense layers) aggregate information from the previous layers to make final predictions. They capture complex interactions and patterns that might not be spatially localized.\n",
    "\n",
    "**Question 11: What do you mean by PARAMETERS?**\n",
    "\n",
    "In the context of neural networks, parameters refer to the weights and biases associated with the connections between neurons in different layers. These parameters are learned during the training process.\n",
    "\n",
    "**Question 12: What formulas are used to measure these PARAMETERS?**\n",
    "\n",
    "The parameters of a neural network (weights and biases) are learned through backpropagation and optimization algorithms. The exact formulas for parameter updates depend on the chosen optimization method (e.g., stochastic gradient descent, Adam, RMSprop), but they involve calculating gradients and adjusting weights to minimize the loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da06ace6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5fc5a411",
   "metadata": {},
   "source": [
    "**Question 1: Why don't we start all of the weights with zeros?**\n",
    "\n",
    "Starting all weights with zeros would result in symmetry across neurons, leading to symmetric updates during backpropagation. This means all neurons in a layer would update identically, and they'd remain symmetric throughout training. As a result, the network wouldn't be able to learn diverse features and patterns effectively.\n",
    "\n",
    "**Question 2: Why is it beneficial to start weights with a mean zero distribution?**\n",
    "\n",
    "Starting weights with a mean zero distribution (such as Gaussian or truncated normal) helps break symmetry and enables neurons to learn different features. Random initialization provides diversity in updates during training, helping the network learn a wide range of features.\n",
    "\n",
    "**Question 3: What is dilated convolution, and how does it work?**\n",
    "\n",
    "Dilated (or atrous) convolution is a convolutional operation that introduces gaps (dilation) between kernel elements. This allows the kernel to have a larger receptive field without increasing the number of parameters. Dilated convolutions capture contextual information at various scales and are used in tasks like semantic segmentation.\n",
    "\n",
    "**Question 4: What is TRANSPOSED CONVOLUTION, and how does it work?**\n",
    "\n",
    "Transposed convolution, also known as deconvolution or fractionally-strided convolution, is used in tasks like upsampling or generating larger feature maps. It performs a reverse operation of a regular convolution, stretching and padding the input to produce a larger output. It helps recover spatial information lost during pooling or downsampling.\n",
    "\n",
    "**Question 5: Explain Separable convolution**\n",
    "\n",
    "Separable convolution breaks down a standard convolution into two smaller convolutions: one for rows and another for columns. This reduces computation and the number of parameters, as it applies fewer filters per layer. It's commonly used in mobile architectures to reduce complexity while retaining effectiveness.\n",
    "\n",
    "**Question 6: What is depthwise convolution, and how does it work?**\n",
    "\n",
    "Depthwise convolution is a separable convolution where each input channel is convolved with its own set of filters, resulting in multiple output feature maps. It's efficient as it performs a separate convolution for each input channel. However, it lacks interactions between channels.\n",
    "\n",
    "**Question 7: What is Depthwise separable convolution, and how does it work?**\n",
    "\n",
    "Depthwise separable convolution combines depthwise convolution (filtering each input channel separately) with pointwise convolution (1x1 convolution across channels). It first captures spatial features independently and then combines them with pointwise convolution. It reduces computation significantly while preserving accuracy.\n",
    "\n",
    "**Question 8: Capsule networks are what they sound like.**\n",
    "\n",
    "Capsule networks, or CapsNets, are a type of neural network architecture that aims to address some limitations of traditional convolutional networks. They focus on modeling hierarchical relationships between features, utilizing \"capsules\" (groups of neurons) that represent different properties of an entity and their spatial relationships.\n",
    "\n",
    "**Question 9: Why is POOLING such an important operation in CNNs?**\n",
    "\n",
    "Pooling reduces spatial dimensions while retaining important features. It helps the network become more invariant to small translations or distortions in the input data. It also reduces computational load and memory usage, allowing for deeper networks and better generalization.\n",
    "\n",
    "**Question 10: What are receptive fields and how do they work?**\n",
    "\n",
    "Receptive fields in CNNs refer to the area in the input image that each neuron in a layer \"sees.\" It's the region of pixels to which a neuron's weights are connected. Receptive fields grow with each layer due to convolution and pooling operations, enabling neurons to capture increasingly complex patterns and features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "825abc95",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

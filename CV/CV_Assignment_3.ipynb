{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "53d090f2",
   "metadata": {},
   "source": [
    "**Question 1: After each stride-2 conv, why do we double the number of filters?**\n",
    "\n",
    "Doubling the number of filters after each stride-2 convolution helps the network capture more complex and abstract features as the spatial dimensions decrease. This increased complexity allows the network to learn higher-level representations of the data.\n",
    "\n",
    "**Question 2: Why do we use a larger kernel with MNIST (with simple CNN) in the first conv?**\n",
    "\n",
    "Using a larger kernel in the first convolutional layer of a simple CNN for MNIST allows the network to capture larger-scale features. Since MNIST digits are relatively small, larger kernels can help detect more distinctive patterns, edges, and shapes in the input images.\n",
    "\n",
    "**Question 3: What data is saved by ActivationStats for each layer?**\n",
    "\n",
    "The ActivationStats callback records the mean and standard deviation of activations for each layer during training. This information can provide insights into the distribution of activations and potential issues like vanishing or exploding gradients.\n",
    "\n",
    "**Question 4: How do we get a learner's callback after they've completed training?**\n",
    "\n",
    "In fastai, you can use the `Learner`'s `fit` method with the `callbacks` parameter to include a callback that executes after training completes. For example: `learn.fit(epochs, callbacks=[MyCallback()])`.\n",
    "\n",
    "**Question 5: What are the drawbacks of activations above zero?**\n",
    "\n",
    "Activations above zero can lead to exploding gradients during training, making the optimization process unstable. This can hinder convergence and make it difficult for the model to learn effectively.\n",
    "\n",
    "**Question 6: Draw up the benefits and drawbacks of practicing in larger batches?**\n",
    "\n",
    "Benefits of larger batches:\n",
    "- Faster training on hardware with parallel processing capabilities.\n",
    "- More stable gradient updates due to larger sample size.\n",
    "\n",
    "Drawbacks of larger batches:\n",
    "- Increased memory usage.\n",
    "- Larger batch sizes might not generalize well to smaller batches during inference.\n",
    "- Can sometimes converge to flatter minima rather than more optimal solutions.\n",
    "\n",
    "**Question 7: Why should we avoid starting training with a high learning rate?**\n",
    "\n",
    "Starting training with a high learning rate can lead to rapid and unstable changes in the model's weights. This can cause the optimization process to overshoot the optimal solution, preventing the model from converging properly.\n",
    "\n",
    "**Question 8: What are the pros of studying with a high rate of learning?**\n",
    "\n",
    "Using a high learning rate at the beginning of training can help the model escape local minima and find more promising regions of the loss landscape. This can accelerate the initial phase of learning, especially when combined with techniques like learning rate annealing.\n",
    "\n",
    "**Question 9: Why do we want to end the training with a low learning rate?**\n",
    "\n",
    "Ending training with a low learning rate helps the model fine-tune its parameters in the later stages. This gradual reduction in learning rate allows the model to make small, precise adjustments to reach an optimal solution while avoiding overshooting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9978f484",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

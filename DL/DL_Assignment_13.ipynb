{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5df7549d",
   "metadata": {},
   "source": [
    "### 1. Why is it generally preferable to use a Logistic Regression classifier rather than a classical Perceptron?\n",
    "**Answer**: Logistic Regression provides probabilities as output, which gives more nuanced information about the confidence of the model's predictions. Logistic Regression is also easier to interpret, can be updated in a more fine-grained way during training due to the nature of its cost function, and tends to work better for non-linearly separable data. The Perceptron, on the other hand, makes hard binary decisions and can only model linearly separable data.\n",
    "\n",
    "To make a Perceptron equivalent to a Logistic Regression classifier, you could replace the step function used in the Perceptron with a logistic (sigmoid) activation function and train it using a log-likelihood cost function.\n",
    "\n",
    "### 2. Why was the logistic activation function a key ingredient in training the first MLPs?\n",
    "**Answer**: The logistic activation function was essential because it is differentiable. This allowed the use of gradient-based optimization methods like backpropagation for training MLPs. It also helps to introduce non-linearity into the model, making it capable of learning from error and adapting during training.\n",
    "\n",
    "### 3. Name three popular activation functions. Can you draw them?\n",
    "**Answer**: Three popular activation functions are ReLU (Rectified Linear Unit), Sigmoid, and Tanh. \n",
    "- ReLU: \\( $f(x) = max(0, x)$ \\)\n",
    "<img src = \"https://www.nomidl.com/wp-content/uploads/2022/04/image-10.png\">\n",
    "- Sigmoid: \\( $f(x) = \\frac{1}{1+e^{-x}}$ \\)\n",
    "<img src = \"https://www.nomidl.com/wp-content/uploads/2022/04/image-11.png\">\n",
    "- Tanh: \\( $f(x) = \\tanh(x)$ \\)\n",
    "<img src = \"https://www.baeldung.com/wp-content/uploads/sites/4/2022/02/tanh.png\">\n",
    "\n",
    "\n",
    "### 4. MLP with specific architecture: Questions about shapes and equations\n",
    "- **Shape of input matrix \\( X \\)**: [N, 10] where N is the number of samples.\n",
    "- **Shape of hidden layer's weight vector \\( Wh \\)**: [10, 50]\n",
    "- **Shape of its bias vector \\( bh \\)**: [50]\n",
    "- **Shape of output layer's weight vector \\( Wo \\)**: [50, 3]\n",
    "- **Shape of its bias vector \\( bo \\)**: [3]\n",
    "- **Shape of network's output matrix \\( Y \\)**: [N, 3]\n",
    "  \n",
    "**Equation**:  \n",
    "\\[ $Y = ReLU(ReLU(X \\cdot Wh + bh) \\cdot Wo + bo)$ \\]\n",
    "\n",
    "### 5. Neurons in the output layer for specific tasks\n",
    "**Answer**: For spam or ham classification, you need just 1 neuron in the output layer using the sigmoid activation function. For MNIST, you need 10 neurons in the output layer and typically use the softmax activation function.\n",
    "\n",
    "### 6. What is backpropagation and how does it work? What is the difference between backpropagation and reverse-mode autodiff?\n",
    "**Answer**: Backpropagation is an algorithm for efficiently computing gradients of the loss function with respect to the weights of the network, which are needed to update the weights to minimize the loss. It works by applying the chain rule of calculus in a clever way to propagate error backwards from the output to the input layer. Reverse-mode autodiff is a more general technique for efficiently computing gradients for any computational graph and is often used as a part of backpropagation.\n",
    "\n",
    "### 7. Can you list all the hyperparameters you can tweak in an MLP? If the MLP overfits the training data, how could you tweak these hyperparameters to try to solve the problem?\n",
    "**Answer**: Hyperparameters in an MLP include the number of hidden layers, the number of neurons in each hidden layer, learning rate, batch size, activation functions, regularization methods, and many others. To combat overfitting, you can reduce the number of hidden layers or neurons, apply dropout, or use techniques like L1/L2 regularization.\n",
    "\n",
    "### 8. Train a deep MLP on the MNIST dataset and see if you can get over 98% precision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e3cc281a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1500/1500 [==============================] - 8s 5ms/step - loss: 0.3678 - accuracy: 0.8884 - val_loss: 0.1634 - val_accuracy: 0.9518\n",
      "Epoch 2/10\n",
      "  25/1500 [..............................] - ETA: 6s - loss: 0.1948 - accuracy: 0.9450"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shuklas\\AppData\\Local\\anaconda3\\lib\\site-packages\\keras\\src\\engine\\training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1500/1500 [==============================] - 5s 4ms/step - loss: 0.1801 - accuracy: 0.9474 - val_loss: 0.1110 - val_accuracy: 0.9672\n",
      "Epoch 3/10\n",
      "1500/1500 [==============================] - 7s 5ms/step - loss: 0.1341 - accuracy: 0.9589 - val_loss: 0.1008 - val_accuracy: 0.9693\n",
      "Epoch 4/10\n",
      "1500/1500 [==============================] - 7s 4ms/step - loss: 0.1139 - accuracy: 0.9653 - val_loss: 0.0878 - val_accuracy: 0.9732\n",
      "Epoch 5/10\n",
      "1500/1500 [==============================] - 7s 4ms/step - loss: 0.1015 - accuracy: 0.9691 - val_loss: 0.0959 - val_accuracy: 0.9724\n",
      "Epoch 6/10\n",
      "1500/1500 [==============================] - 6s 4ms/step - loss: 0.0901 - accuracy: 0.9720 - val_loss: 0.0984 - val_accuracy: 0.9725\n",
      "Epoch 7/10\n",
      "1500/1500 [==============================] - 7s 4ms/step - loss: 0.0833 - accuracy: 0.9742 - val_loss: 0.0854 - val_accuracy: 0.9737\n",
      "Epoch 8/10\n",
      "1500/1500 [==============================] - 7s 5ms/step - loss: 0.0719 - accuracy: 0.9763 - val_loss: 0.0869 - val_accuracy: 0.9764\n",
      "Epoch 9/10\n",
      "1500/1500 [==============================] - 7s 4ms/step - loss: 0.0665 - accuracy: 0.9784 - val_loss: 0.0904 - val_accuracy: 0.9764\n",
      "Epoch 10/10\n",
      "1500/1500 [==============================] - 6s 4ms/step - loss: 0.0656 - accuracy: 0.9793 - val_loss: 0.0806 - val_accuracy: 0.9788\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 0.0674 - accuracy: 0.9812\n",
      "Test accuracy: 98.12%\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Load MNIST data\n",
    "mnist = tf.keras.datasets.mnist\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0  # Normalize to [0, 1]\n",
    "\n",
    "# Flatten the data\n",
    "x_train = x_train.reshape(-1, 28*28)\n",
    "x_test = x_test.reshape(-1, 28*28)\n",
    "\n",
    "# Create the MLP model\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Input(shape=(28*28,)),\n",
    "    tf.keras.layers.Dense(128, activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.2),\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.2),\n",
    "    tf.keras.layers.Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Create checkpoints\n",
    "checkpoint_cb = tf.keras.callbacks.ModelCheckpoint(\"mnist_model.h5\", save_best_only=True)\n",
    "\n",
    "# Use TensorBoard\n",
    "tensorboard_cb = tf.keras.callbacks.TensorBoard(log_dir=\"./logs\")\n",
    "\n",
    "# Train the model\n",
    "model.fit(x_train, y_train, epochs=10, validation_split=0.2,\n",
    "          callbacks=[checkpoint_cb, tensorboard_cb])\n",
    "\n",
    "# Evaluate the model\n",
    "test_loss, test_acc = model.evaluate(x_test, y_test)\n",
    "\n",
    "print(f\"Test accuracy: {test_acc * 100:.2f}%\")\n",
    "\n",
    "# To start TensorBoard, run the following command in your terminal:\n",
    "# tensorboard --logdir=./logs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59fde74a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

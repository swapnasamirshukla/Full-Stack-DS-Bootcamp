{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9525d6e5",
   "metadata": {},
   "source": [
    "### 1. What are the pros and cons of using a stateful RNN versus a stateless RNN?\n",
    "\n",
    "#### Pros of Stateful RNNs:\n",
    "- **Memory**: They remember their state between batches, enabling them to handle long sequences effectively.\n",
    "- **Efficiency**: Can be more computationally efficient as they avoid redundant calculations across overlapping parts of sequences.\n",
    "\n",
    "#### Cons of Stateful RNNs:\n",
    "- **Complexity**: More complex to implement and manage.\n",
    "- **Batch Dependence**: The order of data matters, making it more difficult to prepare data and train the model.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Why do people use Encoder–Decoder RNNs rather than plain sequence-to-sequence RNNs for automatic translation?\n",
    "\n",
    "An Encoder–Decoder RNN structure allows the model to first encode the source sequence into a fixed-length context vector. The Decoder then uses this context to produce the target sequence. This separation provides several advantages:\n",
    "- **Flexibility**: Can handle sequences of different lengths and different purposes (e.g., different languages in translation).\n",
    "- **Performance**: Often yields better performance as the tasks of encoding and decoding can specialize.\n",
    "\n",
    "---\n",
    "\n",
    "### 3. How can you deal with variable-length input sequences? What about variable-length output sequences?\n",
    "\n",
    "#### Variable-length Input Sequences:\n",
    "- **Padding**: Pad shorter sequences with zeros (or another value) so they match the longest sequence.\n",
    "- **Bucketing**: Group sequences of similar lengths together and pad them to the maximum length within each bucket.\n",
    "\n",
    "#### Variable-length Output Sequences:\n",
    "- **Dynamic Decoding**: In sequence-to-sequence models, use special end-of-sequence tokens to indicate when the model should stop producing output.\n",
    "- **Length Prediction**: Some models predict the length of the output sequence as an additional task.\n",
    "\n",
    "---\n",
    "\n",
    "### 4. What is beam search and why would you use it? What tool can you use to implement it?\n",
    "\n",
    "Beam search is a heuristic search algorithm that explores the most promising sequences, maintaining a fixed number of the best partial sequences (the \"beam width\"). It's often used in sequence-to-sequence problems like machine translation.\n",
    "\n",
    "**Why Use It?**\n",
    "- It offers a good trade-off between computational cost and solution quality compared to greedy or exhaustive search.\n",
    "\n",
    "**Tools to Implement**:\n",
    "- Libraries like TensorFlow and PyTorch can be used to implement beam search manually.\n",
    "  \n",
    "---\n",
    "\n",
    "### 5. What is an attention mechanism? How does it help?\n",
    "\n",
    "Attention mechanisms allow a model to focus on specific parts of the input when producing the output. This is especially useful in tasks like machine translation where certain words in the output correspond to specific words in the input.\n",
    "\n",
    "**How Does It Help?**\n",
    "- It solves the problem of fixed-size context vectors in sequence-to-sequence models.\n",
    "- It enables the model to handle long sequences more effectively.\n",
    "\n",
    "---\n",
    "\n",
    "### 6. What is the most important layer in the Transformer architecture? What is its purpose?\n",
    "\n",
    "The most important layer in the Transformer architecture is the Multi-Head Attention layer. \n",
    "\n",
    "**Purpose**: \n",
    "- It allows the model to attend to different parts of the input sequence simultaneously.\n",
    "- It enables the parallelization of attention computation, making training more efficient.\n",
    "\n",
    "---\n",
    "\n",
    "### 7. When would you need to use sampled softmax?\n",
    "\n",
    "Sampled Softmax is typically used when you have a large vocabulary and computing the full softmax becomes computationally expensive. It approximates the full softmax by only considering a random sample of negative classes when computing the loss.\n",
    "\n",
    "It is especially useful in language models, machine translation, and other tasks with a large output space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "808f7008",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9254b6b0",
   "metadata": {},
   "source": [
    "**1. Is it OK to initialize all the weights to the same value as long as that value is selected randomly using He initialization?**\n",
    "\n",
    "**Answer**: No, it's not OK. Initializing all weights to the same value, even if that value is drawn from a distribution like He initialization, will lead to the same gradients during backpropagation. This means that all neurons in a given layer will always get updated by the same amount, effectively making them identical throughout training. The network won't be able to exploit the power of a deep architecture. Diverse initial weights break symmetry and allow neurons to learn different features.\n",
    "\n",
    "**2. Is it OK to initialize the bias terms to 0?**\n",
    "\n",
    "**Answer**: Yes, it's generally acceptable to initialize bias terms to 0. This is because the asymmetry breaking is typically provided by the small random numbers in the weights. Thus, starting the biases at zero usually won't hinder learning.\n",
    "\n",
    "**3. Name three advantages of the SELU activation function over ReLU.**\n",
    "\n",
    "**Answer**:\n",
    "- **Self-normalization**: When used with the correct initialization and network architecture, SELU activation functions tend to output values that preserve a mean of 0 and standard deviation of 1 during training, which can help mitigate the vanishing/exploding gradients problem.\n",
    "  \n",
    "- **No dying units**: Unlike ReLU, which can have neurons that stop outputting anything other than 0 (dying ReLUs), SELU doesn't have this problem as it's smooth and differentiable everywhere.\n",
    "\n",
    "- **Mitigates vanishing gradients**: The negative slope for values less than 0 in SELU helps mitigate the vanishing gradients problem, which ReLUs can sometimes exacerbate for negative input values.\n",
    "\n",
    "**4. In which cases would you want to use each of the following activation functions: SELU, leaky ReLU (and its variants), ReLU, tanh, logistic, and softmax?**\n",
    "\n",
    "**Answer**:\n",
    "- **SELU**: Useful for deep neural networks as it helps keep the activations' mean and variance close to 0 and 1, respectively. Works best with a specific architecture (e.g., purely dense layers).\n",
    "\n",
    "- **Leaky ReLU and its variants**: Useful when there's a concern about dying ReLUs. Variants like Parametric or Exponential Leaky ReLU provide more flexibility and can sometimes outperform the standard Leaky ReLU.\n",
    "\n",
    "- **ReLU**: A good default for most situations in feedforward deep networks due to its simplicity and efficiency. However, it can be problematic if there are many dying ReLUs.\n",
    "\n",
    "- **tanh**: Useful when outputs need to be scaled between -1 and 1. Common in older architectures and in certain RNN structures.\n",
    "\n",
    "- **logistic (sigmoid)**: Often found in binary classification tasks as the activation function for the output layer. Also used in older architectures.\n",
    "\n",
    "- **softmax**: Specifically used in the output layer for multi-class classification problems. Outputs a probability distribution over N classes.\n",
    "\n",
    "**5. What may happen if you set the momentum hyperparameter too close to 1 (e.g., 0.99999) when using an SGD optimizer?**\n",
    "\n",
    "**Answer**: Setting the momentum hyperparameter too close to 1 can cause the optimizer to become very sensitive to the most recent gradients and might overshoot a lot. This can lead to oscillations or divergence and may prevent the optimizer from settling into a minimum.\n",
    "\n",
    "**6. Name three ways you can produce a sparse model.**\n",
    "\n",
    "**Answer**: \n",
    "- **L1 Regularization**: This imposes a penalty on the absolute values of the weights. This tends to produce sparse weight matrices where many weights are exactly zero.\n",
    "\n",
    "- **Pruning**: After training a model, small-weight connections can be pruned (set to zero), and the model can be fine-tuned further with the pruned architecture.\n",
    "\n",
    "- **Using specialized techniques or architectures**: Such as the TensorFlow Model Optimization Toolkit (TF-MOT), which provides tools to produce sparse models.\n",
    "\n",
    "**7. Does dropout slow down training? Does it slow down inference (i.e., making predictions on new instances)? What about MC Dropout?**\n",
    "\n",
    "**Answer**: \n",
    "- **Dropout**: It does slow down training since, at each training step, it randomly drops a fraction of the inputs. However, it doesn't slow down inference. During inference, dropout layers are turned off, and all neurons are used.\n",
    "\n",
    "- **MC Dropout**: MC Dropout does slow down inference. This is because, even during inference, dropout is kept active, and the network needs multiple forward passes to obtain an averaged prediction. The number of forward passes depends on how many samples you decide to use for the Monte Carlo approximation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0480b97e",
   "metadata": {},
   "source": [
    "**8. Practice training a deep neural network on the CIFAR10 image dataset:**\n",
    "\n",
    "**a. Build a DNN with 20 hidden layers of 100 neurons each (that’s too many, but it’s the point of this exercise). Use He initialization and the ELU activation function.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8bc25a76",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "# Build the DNN\n",
    "model = keras.models.Sequential()\n",
    "\n",
    "# Input layer\n",
    "model.add(keras.layers.Flatten(input_shape=[32, 32, 3]))\n",
    "\n",
    "# Add 20 hidden layers of 100 neurons each\n",
    "for _ in range(20):\n",
    "    model.add(keras.layers.Dense(100, activation=\"elu\", kernel_initializer=\"he_normal\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41b0abff",
   "metadata": {},
   "source": [
    "\n",
    "**b. Using Nadam optimization and early stopping, train the network on the CIFAR10 dataset. You can load it with keras.datasets.cifar10.load_​data(). The dataset is composed of 60,000 32 × 32–pixel color images (50,000 for training, 10,000 for testing) with 10 classes, so you’ll need a softmax output layer with 10 neurons. Remember to search for the right learning rate each time you change the model’s architecture or hyperparameters.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a15f2f99",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Nadam.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1407/1407 [==============================] - 22s 9ms/step - loss: 3.9557 - accuracy: 0.2214 - val_loss: 1.9603 - val_accuracy: 0.2768\n",
      "Epoch 2/100\n",
      "1407/1407 [==============================] - 12s 9ms/step - loss: 1.9233 - accuracy: 0.2938 - val_loss: 1.9251 - val_accuracy: 0.2958\n",
      "Epoch 3/100\n",
      "1407/1407 [==============================] - 12s 9ms/step - loss: 1.8611 - accuracy: 0.3223 - val_loss: 1.8375 - val_accuracy: 0.3206\n",
      "Epoch 4/100\n",
      "1407/1407 [==============================] - 12s 9ms/step - loss: 1.8255 - accuracy: 0.3332 - val_loss: 1.9960 - val_accuracy: 0.3032\n",
      "Epoch 5/100\n",
      "1407/1407 [==============================] - 12s 9ms/step - loss: 1.8028 - accuracy: 0.3438 - val_loss: 1.8135 - val_accuracy: 0.3470\n",
      "Epoch 6/100\n",
      "1407/1407 [==============================] - 12s 9ms/step - loss: 1.7722 - accuracy: 0.3540 - val_loss: 1.7953 - val_accuracy: 0.3534\n",
      "Epoch 7/100\n",
      "1407/1407 [==============================] - 12s 9ms/step - loss: 1.7509 - accuracy: 0.3664 - val_loss: 1.7467 - val_accuracy: 0.3726\n",
      "Epoch 8/100\n",
      "1407/1407 [==============================] - 12s 9ms/step - loss: 1.7324 - accuracy: 0.3722 - val_loss: 1.7393 - val_accuracy: 0.3782\n",
      "Epoch 9/100\n",
      "1407/1407 [==============================] - 12s 9ms/step - loss: 1.8009 - accuracy: 0.3681 - val_loss: 2.1122 - val_accuracy: 0.1946\n",
      "Epoch 10/100\n",
      "1407/1407 [==============================] - 13s 9ms/step - loss: 1.9295 - accuracy: 0.2733 - val_loss: 1.8518 - val_accuracy: 0.3094\n",
      "Epoch 11/100\n",
      "1407/1407 [==============================] - 12s 9ms/step - loss: 1.8637 - accuracy: 0.3026 - val_loss: 1.8417 - val_accuracy: 0.3006\n",
      "Epoch 12/100\n",
      "1407/1407 [==============================] - 12s 9ms/step - loss: 1.8314 - accuracy: 0.3140 - val_loss: 1.8375 - val_accuracy: 0.3010\n",
      "Epoch 13/100\n",
      "1407/1407 [==============================] - 13s 9ms/step - loss: 1.8114 - accuracy: 0.3261 - val_loss: 1.8508 - val_accuracy: 0.3066\n",
      "Epoch 14/100\n",
      "1407/1407 [==============================] - 13s 9ms/step - loss: 1.8010 - accuracy: 0.3326 - val_loss: 1.8204 - val_accuracy: 0.3268\n",
      "Epoch 15/100\n",
      "1407/1407 [==============================] - 13s 9ms/step - loss: 1.7953 - accuracy: 0.3377 - val_loss: 1.8621 - val_accuracy: 0.3132\n",
      "Epoch 16/100\n",
      "1407/1407 [==============================] - 12s 8ms/step - loss: 1.8617 - accuracy: 0.3020 - val_loss: 1.8771 - val_accuracy: 0.2910\n",
      "Epoch 17/100\n",
      "1407/1407 [==============================] - 10s 7ms/step - loss: 2.7943 - accuracy: 0.2697 - val_loss: 1.9248 - val_accuracy: 0.2608\n",
      "Epoch 18/100\n",
      "1407/1407 [==============================] - 10s 7ms/step - loss: 1.9696 - accuracy: 0.2749 - val_loss: 1.8923 - val_accuracy: 0.2972\n",
      "Epoch 19/100\n",
      "1407/1407 [==============================] - 10s 7ms/step - loss: 1.8597 - accuracy: 0.3047 - val_loss: 1.8387 - val_accuracy: 0.3090\n",
      "Epoch 20/100\n",
      "1407/1407 [==============================] - 10s 7ms/step - loss: 1.8361 - accuracy: 0.3150 - val_loss: 1.8458 - val_accuracy: 0.3066\n",
      "Epoch 21/100\n",
      "1407/1407 [==============================] - 10s 7ms/step - loss: 1.8291 - accuracy: 0.3222 - val_loss: 1.7970 - val_accuracy: 0.3356\n",
      "Epoch 22/100\n",
      "1407/1407 [==============================] - 10s 7ms/step - loss: 1.7990 - accuracy: 0.3345 - val_loss: 1.8565 - val_accuracy: 0.3296\n",
      "Epoch 23/100\n",
      "1407/1407 [==============================] - 10s 7ms/step - loss: 1.7864 - accuracy: 0.3411 - val_loss: 1.7907 - val_accuracy: 0.3388\n",
      "Epoch 24/100\n",
      "1407/1407 [==============================] - 10s 7ms/step - loss: 1.7813 - accuracy: 0.3449 - val_loss: 1.7570 - val_accuracy: 0.3474\n",
      "Epoch 25/100\n",
      "1407/1407 [==============================] - 10s 7ms/step - loss: 1.7710 - accuracy: 0.3478 - val_loss: 1.7781 - val_accuracy: 0.3366\n",
      "Epoch 26/100\n",
      "1407/1407 [==============================] - 10s 7ms/step - loss: 1.7606 - accuracy: 0.3504 - val_loss: 1.8072 - val_accuracy: 0.3222\n",
      "Epoch 27/100\n",
      "1407/1407 [==============================] - 10s 7ms/step - loss: 1.7538 - accuracy: 0.3557 - val_loss: 1.7683 - val_accuracy: 0.3584\n",
      "Epoch 28/100\n",
      "1407/1407 [==============================] - 10s 7ms/step - loss: 1.7458 - accuracy: 0.3563 - val_loss: 1.8301 - val_accuracy: 0.3328\n"
     ]
    }
   ],
   "source": [
    "# Load the CIFAR10 dataset\n",
    "(X_train_full, y_train_full), (X_test, y_test) = keras.datasets.cifar10.load_data()\n",
    "\n",
    "# Split the full training set into a validation set and a (smaller) training set\n",
    "X_train = X_train_full[5000:]\n",
    "y_train = y_train_full[5000:]\n",
    "X_valid = X_train_full[:5000]\n",
    "y_valid = y_train_full[:5000]\n",
    "\n",
    "# Output layer\n",
    "model.add(keras.layers.Dense(10, activation=\"softmax\"))\n",
    "\n",
    "# Compile the model with Nadam optimization\n",
    "optimizer = keras.optimizers.Nadam(lr=5e-5)\n",
    "model.compile(optimizer=optimizer, loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "# Early stopping\n",
    "early_stopping_cb = keras.callbacks.EarlyStopping(patience=20, restore_best_weights=True)\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train, y_train, epochs=100, validation_data=(X_valid, y_valid), callbacks=[early_stopping_cb])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "391303b3",
   "metadata": {},
   "source": [
    "**c. Now try adding Batch Normalization and compare the learning curves: Is it converging faster than before? Does it produce a better model? How does it affect training speed?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d6923ee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear previous model\n",
    "keras.backend.clear_session()\n",
    "\n",
    "model = keras.models.Sequential()\n",
    "model.add(keras.layers.Flatten(input_shape=[32, 32, 3]))\n",
    "\n",
    "for _ in range(20):\n",
    "    model.add(keras.layers.Dense(100, kernel_initializer=\"he_normal\"))\n",
    "    model.add(keras.layers.BatchNormalization())\n",
    "    model.add(keras.layers.Activation(\"elu\"))\n",
    "\n",
    "model.add(keras.layers.Dense(10, activation=\"softmax\"))\n",
    "optimizer = tf.keras.optimizers.legacy.Nadam(lr=5e-5)\n",
    "#optimizer.build(model.trainable_variables)\n",
    "model.compile(optimizer=optimizer, loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "#model.build(input_shape=(None, 32, 32, 3))\n",
    "#optimizer = tf.keras.optimizers.legacy.Nadam(lr=5e-5)\n",
    "#model.compile(optimizer=optimizer, loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "92098d84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1407/1407 [==============================] - 21s 10ms/step - loss: 2.0734 - accuracy: 0.2588 - val_loss: 1.8583 - val_accuracy: 0.3376\n",
      "Epoch 2/100\n",
      "1407/1407 [==============================] - 13s 9ms/step - loss: 1.7973 - accuracy: 0.3554 - val_loss: 1.7586 - val_accuracy: 0.3736\n",
      "Epoch 3/100\n",
      "1407/1407 [==============================] - 13s 10ms/step - loss: 1.6988 - accuracy: 0.3936 - val_loss: 1.6218 - val_accuracy: 0.4268\n",
      "Epoch 4/100\n",
      "1407/1407 [==============================] - 13s 10ms/step - loss: 1.6362 - accuracy: 0.4230 - val_loss: 1.6307 - val_accuracy: 0.4210\n",
      "Epoch 5/100\n",
      "1407/1407 [==============================] - 14s 10ms/step - loss: 1.5949 - accuracy: 0.4320 - val_loss: 1.5536 - val_accuracy: 0.4514\n",
      "Epoch 6/100\n",
      "1407/1407 [==============================] - 13s 10ms/step - loss: 1.5587 - accuracy: 0.4467 - val_loss: 1.5435 - val_accuracy: 0.4436\n",
      "Epoch 7/100\n",
      "1407/1407 [==============================] - 13s 10ms/step - loss: 1.5294 - accuracy: 0.4570 - val_loss: 1.6377 - val_accuracy: 0.4310\n",
      "Epoch 8/100\n",
      "1407/1407 [==============================] - 13s 9ms/step - loss: 1.5015 - accuracy: 0.4669 - val_loss: 1.5383 - val_accuracy: 0.4520\n",
      "Epoch 9/100\n",
      "1407/1407 [==============================] - 14s 10ms/step - loss: 1.4771 - accuracy: 0.4759 - val_loss: 1.5975 - val_accuracy: 0.4258\n",
      "Epoch 10/100\n",
      "1407/1407 [==============================] - 14s 10ms/step - loss: 1.4537 - accuracy: 0.4868 - val_loss: 1.4633 - val_accuracy: 0.4844\n",
      "Epoch 11/100\n",
      "1407/1407 [==============================] - 14s 10ms/step - loss: 1.4330 - accuracy: 0.4934 - val_loss: 1.4820 - val_accuracy: 0.4730\n",
      "Epoch 12/100\n",
      "1407/1407 [==============================] - 14s 10ms/step - loss: 1.4182 - accuracy: 0.4974 - val_loss: 1.4785 - val_accuracy: 0.4732\n",
      "Epoch 13/100\n",
      "1407/1407 [==============================] - 14s 10ms/step - loss: 1.3964 - accuracy: 0.5069 - val_loss: 1.4792 - val_accuracy: 0.4780\n",
      "Epoch 14/100\n",
      "1407/1407 [==============================] - 14s 10ms/step - loss: 1.3840 - accuracy: 0.5111 - val_loss: 1.4467 - val_accuracy: 0.4956\n",
      "Epoch 15/100\n",
      "1407/1407 [==============================] - 14s 10ms/step - loss: 1.3635 - accuracy: 0.5190 - val_loss: 1.5725 - val_accuracy: 0.4502\n",
      "Epoch 16/100\n",
      "1407/1407 [==============================] - 14s 10ms/step - loss: 1.3521 - accuracy: 0.5212 - val_loss: 1.4352 - val_accuracy: 0.4938\n",
      "Epoch 17/100\n",
      "1407/1407 [==============================] - 14s 10ms/step - loss: 1.3406 - accuracy: 0.5235 - val_loss: 1.4642 - val_accuracy: 0.4882\n",
      "Epoch 18/100\n",
      "1407/1407 [==============================] - 14s 10ms/step - loss: 1.3266 - accuracy: 0.5274 - val_loss: 1.5503 - val_accuracy: 0.4582\n",
      "Epoch 19/100\n",
      "1407/1407 [==============================] - 14s 10ms/step - loss: 1.3127 - accuracy: 0.5396 - val_loss: 1.4914 - val_accuracy: 0.4826\n",
      "Epoch 20/100\n",
      "1407/1407 [==============================] - 14s 10ms/step - loss: 1.3006 - accuracy: 0.5382 - val_loss: 1.4101 - val_accuracy: 0.5062\n",
      "Epoch 21/100\n",
      "1407/1407 [==============================] - 13s 10ms/step - loss: 1.2939 - accuracy: 0.5388 - val_loss: 1.4570 - val_accuracy: 0.4842\n",
      "Epoch 22/100\n",
      "1407/1407 [==============================] - 13s 9ms/step - loss: 1.2759 - accuracy: 0.5466 - val_loss: 1.4283 - val_accuracy: 0.5028\n",
      "Epoch 23/100\n",
      "1407/1407 [==============================] - 13s 9ms/step - loss: 1.2661 - accuracy: 0.5504 - val_loss: 1.3983 - val_accuracy: 0.5094\n",
      "Epoch 24/100\n",
      "1407/1407 [==============================] - 13s 10ms/step - loss: 1.2594 - accuracy: 0.5544 - val_loss: 1.6891 - val_accuracy: 0.4500\n",
      "Epoch 25/100\n",
      "1407/1407 [==============================] - 14s 10ms/step - loss: 1.2527 - accuracy: 0.5556 - val_loss: 1.4699 - val_accuracy: 0.4816\n",
      "Epoch 26/100\n",
      "1407/1407 [==============================] - 13s 10ms/step - loss: 1.2401 - accuracy: 0.5615 - val_loss: 1.5414 - val_accuracy: 0.4690\n",
      "Epoch 27/100\n",
      "1407/1407 [==============================] - 13s 9ms/step - loss: 1.2340 - accuracy: 0.5656 - val_loss: 1.4416 - val_accuracy: 0.4934\n",
      "Epoch 28/100\n",
      "1407/1407 [==============================] - 13s 9ms/step - loss: 1.2210 - accuracy: 0.5681 - val_loss: 1.4561 - val_accuracy: 0.5056\n",
      "Epoch 29/100\n",
      "1407/1407 [==============================] - 13s 10ms/step - loss: 1.2157 - accuracy: 0.5694 - val_loss: 1.4169 - val_accuracy: 0.5088\n",
      "Epoch 30/100\n",
      "1407/1407 [==============================] - 13s 10ms/step - loss: 1.1979 - accuracy: 0.5741 - val_loss: 1.4600 - val_accuracy: 0.4880\n",
      "Epoch 31/100\n",
      "1407/1407 [==============================] - 13s 9ms/step - loss: 1.1987 - accuracy: 0.5770 - val_loss: 1.4675 - val_accuracy: 0.4886\n",
      "Epoch 32/100\n",
      "1407/1407 [==============================] - 13s 10ms/step - loss: 1.1915 - accuracy: 0.5759 - val_loss: 1.5150 - val_accuracy: 0.4784\n",
      "Epoch 33/100\n",
      "1407/1407 [==============================] - 13s 9ms/step - loss: 1.1835 - accuracy: 0.5808 - val_loss: 1.4851 - val_accuracy: 0.4846\n",
      "Epoch 34/100\n",
      "1407/1407 [==============================] - 13s 10ms/step - loss: 1.1786 - accuracy: 0.5804 - val_loss: 1.5201 - val_accuracy: 0.4796\n",
      "Epoch 35/100\n",
      "1407/1407 [==============================] - 13s 10ms/step - loss: 1.1599 - accuracy: 0.5859 - val_loss: 1.3967 - val_accuracy: 0.5112\n",
      "Epoch 36/100\n",
      "1407/1407 [==============================] - 13s 9ms/step - loss: 1.1594 - accuracy: 0.5899 - val_loss: 1.4864 - val_accuracy: 0.4860\n",
      "Epoch 37/100\n",
      "1407/1407 [==============================] - 13s 9ms/step - loss: 1.1537 - accuracy: 0.5926 - val_loss: 1.5698 - val_accuracy: 0.4646\n",
      "Epoch 38/100\n",
      "1407/1407 [==============================] - 13s 9ms/step - loss: 1.1475 - accuracy: 0.5921 - val_loss: 1.4912 - val_accuracy: 0.4820\n",
      "Epoch 39/100\n",
      "1407/1407 [==============================] - 13s 9ms/step - loss: 1.1352 - accuracy: 0.6001 - val_loss: 1.4679 - val_accuracy: 0.4910\n",
      "Epoch 40/100\n",
      "1407/1407 [==============================] - 13s 9ms/step - loss: 1.1310 - accuracy: 0.5986 - val_loss: 1.5508 - val_accuracy: 0.4696\n",
      "Epoch 41/100\n",
      "1407/1407 [==============================] - 13s 10ms/step - loss: 1.1255 - accuracy: 0.6008 - val_loss: 1.4424 - val_accuracy: 0.5058\n",
      "Epoch 42/100\n",
      "1407/1407 [==============================] - 13s 9ms/step - loss: 1.1191 - accuracy: 0.6044 - val_loss: 1.4290 - val_accuracy: 0.5048\n",
      "Epoch 43/100\n",
      "1407/1407 [==============================] - 13s 9ms/step - loss: 1.1126 - accuracy: 0.6062 - val_loss: 1.4558 - val_accuracy: 0.5014\n",
      "Epoch 44/100\n",
      "1407/1407 [==============================] - 14s 10ms/step - loss: 1.1016 - accuracy: 0.6086 - val_loss: 1.4350 - val_accuracy: 0.5016\n",
      "Epoch 45/100\n",
      "1407/1407 [==============================] - 13s 9ms/step - loss: 1.0929 - accuracy: 0.6116 - val_loss: 1.4701 - val_accuracy: 0.4974\n",
      "Epoch 46/100\n",
      "1407/1407 [==============================] - 13s 9ms/step - loss: 1.0867 - accuracy: 0.6163 - val_loss: 1.4390 - val_accuracy: 0.5076\n",
      "Epoch 47/100\n",
      "1407/1407 [==============================] - 13s 9ms/step - loss: 1.0804 - accuracy: 0.6165 - val_loss: 1.4237 - val_accuracy: 0.5154\n",
      "Epoch 48/100\n",
      "1407/1407 [==============================] - 13s 9ms/step - loss: 1.0743 - accuracy: 0.6192 - val_loss: 1.5017 - val_accuracy: 0.4892\n",
      "Epoch 49/100\n",
      "1407/1407 [==============================] - 13s 9ms/step - loss: 1.0746 - accuracy: 0.6198 - val_loss: 1.4321 - val_accuracy: 0.5102\n",
      "Epoch 50/100\n",
      "1407/1407 [==============================] - 13s 10ms/step - loss: 1.0711 - accuracy: 0.6178 - val_loss: 1.4136 - val_accuracy: 0.5156\n",
      "Epoch 51/100\n",
      "1407/1407 [==============================] - 13s 9ms/step - loss: 1.0547 - accuracy: 0.6287 - val_loss: 1.5757 - val_accuracy: 0.4732\n",
      "Epoch 52/100\n",
      "1407/1407 [==============================] - 13s 10ms/step - loss: 1.0549 - accuracy: 0.6267 - val_loss: 1.4761 - val_accuracy: 0.5044\n",
      "Epoch 53/100\n",
      "1407/1407 [==============================] - 14s 10ms/step - loss: 1.0504 - accuracy: 0.6275 - val_loss: 1.5834 - val_accuracy: 0.4800\n",
      "Epoch 54/100\n",
      "1407/1407 [==============================] - 14s 10ms/step - loss: 1.0433 - accuracy: 0.6286 - val_loss: 1.5567 - val_accuracy: 0.4804\n",
      "Epoch 55/100\n",
      "1407/1407 [==============================] - 14s 10ms/step - loss: 1.0481 - accuracy: 0.6303 - val_loss: 1.4188 - val_accuracy: 0.5242\n"
     ]
    }
   ],
   "source": [
    "history_bn = model.fit(X_train, y_train, epochs=100, validation_data=(X_valid, y_valid), callbacks=[early_stopping_cb])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1823b79",
   "metadata": {},
   "source": [
    "**d. Try replacing Batch Normalization with SELU, and make the necessary adjustements to ensure the network self-normalizes (i.e., standardize the input features, use LeCun normal initialization, make sure the DNN contains only a sequence of dense layers, etc.).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "99adde38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear previous model\n",
    "keras.backend.clear_session()\n",
    "\n",
    "# Standardize the input\n",
    "pixel_means = X_train.mean(axis=0, keepdims=True)\n",
    "pixel_stds = X_train.std(axis=0, keepdims=True)\n",
    "X_train_scaled = (X_train - pixel_means) / pixel_stds\n",
    "X_valid_scaled = (X_valid - pixel_means) / pixel_stds\n",
    "X_test_scaled = (X_test - pixel_means) / pixel_stds\n",
    "\n",
    "model = keras.models.Sequential()\n",
    "model.add(keras.layers.Flatten(input_shape=[32, 32, 3]))\n",
    "\n",
    "for _ in range(20):\n",
    "    model.add(keras.layers.Dense(100, activation=\"selu\", kernel_initializer=\"lecun_normal\"))\n",
    "\n",
    "model.add(keras.layers.Dense(10, activation=\"softmax\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e705d73f",
   "metadata": {},
   "source": [
    "**e. Try regularizing the model with alpha dropout. Then, without retraining your model,see if you can achieve better accuracy using MC Dropout.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1622708d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1407/1407 [==============================] - 11s 6ms/step - loss: 3.3123 - accuracy: 0.3426 - val_loss: 2.1716 - val_accuracy: 0.4076\n",
      "Epoch 2/100\n",
      "1407/1407 [==============================] - 8s 6ms/step - loss: 3.2404 - accuracy: 0.4022 - val_loss: 2.1388 - val_accuracy: 0.4432\n",
      "Epoch 3/100\n",
      "1407/1407 [==============================] - 8s 6ms/step - loss: 3.1896 - accuracy: 0.4340 - val_loss: 2.1745 - val_accuracy: 0.4404\n",
      "Epoch 4/100\n",
      "1407/1407 [==============================] - 8s 6ms/step - loss: 3.1552 - accuracy: 0.4532 - val_loss: 2.1373 - val_accuracy: 0.4590\n",
      "Epoch 5/100\n",
      "1407/1407 [==============================] - 8s 6ms/step - loss: 3.1159 - accuracy: 0.4680 - val_loss: 2.1820 - val_accuracy: 0.4650\n",
      "Epoch 6/100\n",
      "1407/1407 [==============================] - 8s 6ms/step - loss: 3.0976 - accuracy: 0.4814 - val_loss: 2.1734 - val_accuracy: 0.4806\n",
      "Epoch 7/100\n",
      "1407/1407 [==============================] - 8s 6ms/step - loss: 3.0916 - accuracy: 0.4957 - val_loss: 2.2389 - val_accuracy: 0.4776\n",
      "Epoch 8/100\n",
      "1407/1407 [==============================] - 8s 5ms/step - loss: 3.0804 - accuracy: 0.5060 - val_loss: 2.3053 - val_accuracy: 0.4772\n",
      "Epoch 9/100\n",
      "1407/1407 [==============================] - 8s 6ms/step - loss: 3.0510 - accuracy: 0.5181 - val_loss: 2.3212 - val_accuracy: 0.4836\n",
      "Epoch 10/100\n",
      "1407/1407 [==============================] - 8s 6ms/step - loss: 3.0207 - accuracy: 0.5282 - val_loss: 2.3005 - val_accuracy: 0.4884\n",
      "Epoch 11/100\n",
      "1407/1407 [==============================] - 8s 6ms/step - loss: 2.9973 - accuracy: 0.5403 - val_loss: 2.3299 - val_accuracy: 0.4960\n",
      "Epoch 12/100\n",
      "1407/1407 [==============================] - 8s 6ms/step - loss: 3.0098 - accuracy: 0.5487 - val_loss: 2.4059 - val_accuracy: 0.4974\n",
      "Epoch 13/100\n",
      "1407/1407 [==============================] - 8s 6ms/step - loss: 2.9997 - accuracy: 0.5571 - val_loss: 2.5230 - val_accuracy: 0.4974\n",
      "Epoch 14/100\n",
      "1407/1407 [==============================] - 8s 6ms/step - loss: 2.9511 - accuracy: 0.5692 - val_loss: 2.6032 - val_accuracy: 0.4914\n",
      "Epoch 15/100\n",
      "1407/1407 [==============================] - 8s 6ms/step - loss: 2.9689 - accuracy: 0.5781 - val_loss: 2.6606 - val_accuracy: 0.4942\n",
      "Epoch 16/100\n",
      "1407/1407 [==============================] - 8s 6ms/step - loss: 2.9084 - accuracy: 0.5841 - val_loss: 2.7195 - val_accuracy: 0.4902\n",
      "Epoch 17/100\n",
      "1407/1407 [==============================] - 8s 6ms/step - loss: 2.9056 - accuracy: 0.5921 - val_loss: 2.7605 - val_accuracy: 0.4992\n",
      "Epoch 18/100\n",
      "1407/1407 [==============================] - 8s 6ms/step - loss: 2.9362 - accuracy: 0.5978 - val_loss: 2.8604 - val_accuracy: 0.4948\n",
      "Epoch 19/100\n",
      "1407/1407 [==============================] - 8s 6ms/step - loss: 2.9044 - accuracy: 0.6053 - val_loss: 2.9491 - val_accuracy: 0.4904\n",
      "Epoch 20/100\n",
      "1407/1407 [==============================] - 8s 6ms/step - loss: 2.8848 - accuracy: 0.6110 - val_loss: 2.9682 - val_accuracy: 0.5008\n",
      "Epoch 21/100\n",
      "1407/1407 [==============================] - 8s 6ms/step - loss: 2.9104 - accuracy: 0.6152 - val_loss: 3.0942 - val_accuracy: 0.4924\n",
      "Epoch 22/100\n",
      "1407/1407 [==============================] - 8s 6ms/step - loss: 2.8636 - accuracy: 0.6220 - val_loss: 3.1228 - val_accuracy: 0.4978\n",
      "Epoch 23/100\n",
      "1407/1407 [==============================] - 8s 6ms/step - loss: 2.8754 - accuracy: 0.6246 - val_loss: 3.2442 - val_accuracy: 0.4914\n",
      "Epoch 24/100\n",
      "1407/1407 [==============================] - 8s 6ms/step - loss: 2.8848 - accuracy: 0.6323 - val_loss: 3.3024 - val_accuracy: 0.5004\n",
      "0.4597\n"
     ]
    }
   ],
   "source": [
    "# Add AlphaDropout for regularization\n",
    "model.add(keras.layers.AlphaDropout(rate=0.1))\n",
    "\n",
    "# Train the model\n",
    "model.compile(optimizer=optimizer, loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "history_alpha = model.fit(X_train_scaled, y_train, epochs=100, validation_data=(X_valid_scaled, y_valid), callbacks=[early_stopping_cb])\n",
    "\n",
    "# Use MC Dropout for predictions\n",
    "y_probas = np.stack([model(X_test_scaled, training=True) for sample in range(100)])\n",
    "y_proba = y_probas.mean(axis=0)\n",
    "y_pred = np.argmax(y_proba, axis=1)\n",
    "\n",
    "accuracy = np.sum(y_pred == y_test[:, 0]) / len(y_test)\n",
    "print(accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

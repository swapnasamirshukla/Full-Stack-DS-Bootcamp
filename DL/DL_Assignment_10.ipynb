{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9b8d72f4",
   "metadata": {},
   "source": [
    "### 1. What does a SavedModel contain? How do you inspect its content?\n",
    "\n",
    "A SavedModel in TensorFlow contains:\n",
    "- A serialized description of the computation graph (in a `saved_model.pb` file).\n",
    "- Checkpoint files containing the model's weights.\n",
    "- Metadata and signatures describing inputs and outputs.\n",
    "\n",
    "**Inspecting Content**:\n",
    "- You can use TensorFlow's `saved_model_cli` tool to inspect the contents.\n",
    "\n",
    "  ```bash\n",
    "  saved_model_cli show --dir /path/to/saved_model --all\n",
    "  ```\n",
    "\n",
    "---\n",
    "\n",
    "### 2. When should you use TF Serving? What are its main features? What are some tools you can use to deploy it?\n",
    "\n",
    "**When to Use**: \n",
    "- When you need to deploy a machine learning model in a production environment.\n",
    "- When you need high-throughput, low-latency serving.\n",
    "\n",
    "**Main Features**:\n",
    "- Batching capabilities\n",
    "- Monitoring\n",
    "- Versioning\n",
    "- RESTful and gRPC APIs\n",
    "\n",
    "**Tools for Deployment**:\n",
    "- Docker\n",
    "- Kubernetes\n",
    "\n",
    "---\n",
    "\n",
    "### 3. How do you deploy a model across multiple TF Serving instances?\n",
    "\n",
    "You can deploy a model across multiple TF Serving instances by using orchestration tools like Kubernetes. You can specify the number of replicas and use a load balancer to distribute incoming requests among the instances.\n",
    "\n",
    "---\n",
    "\n",
    "### 4. When should you use the gRPC API rather than the REST API to query a model served by TF Serving?\n",
    "\n",
    "Use gRPC API when:\n",
    "- You need better performance and lower latency.\n",
    "- Your application is written in a language that has good support for gRPC.\n",
    "- You require advanced features like streaming, authentication, etc.\n",
    "\n",
    "---\n",
    "\n",
    "### 5. What are the different ways TFLite reduces a modelâ€™s size to make it run on a mobile or embedded device?\n",
    "\n",
    "- **Quantization**: Reduces the numerical precision of the model's weights.\n",
    "- **Pruning**: Removes less important weights.\n",
    "- **Optimized Kernels**: Uses optimized operations that are computationally efficient.\n",
    "- **Model Simplification**: Simplifies the architecture of the model.\n",
    "\n",
    "---\n",
    "\n",
    "### 6. What is quantization-aware training, and why would you need it?\n",
    "\n",
    "Quantization-aware training simulates the effects of quantization (reduced numerical precision) during the training process. This helps the model adapt to the loss of precision, which makes it perform better when actually quantized.\n",
    "\n",
    "**Why You'd Need It**:\n",
    "- To maintain good performance when deploying a quantized model.\n",
    "- To make the model size smaller and more efficient for deployment on edge devices.\n",
    "\n",
    "---\n",
    "\n",
    "### 7. What are model parallelism and data parallelism? Why is the latter generally recommended?\n",
    "\n",
    "**Model Parallelism**: The model is split across multiple GPUs or other devices. Each device computes a portion of the model.\n",
    "\n",
    "**Data Parallelism**: Each device computes the whole model but on different subsets of the data.\n",
    "\n",
    "**Why Data Parallelism is Generally Recommended**:\n",
    "- Easier to implement.\n",
    "- Scales better with more devices.\n",
    "- Less communication overhead between devices.\n",
    "\n",
    "---\n",
    "\n",
    "### 8. When training a model across multiple servers, what distribution strategies can you use? How do you choose which one to use?\n",
    "\n",
    "**Distribution Strategies**:\n",
    "- **MirroredStrategy**: Data parallelism on a single machine with multiple GPUs.\n",
    "- **MultiWorkerMirroredStrategy**: Data parallelism across multiple machines.\n",
    "- **TPUStrategy**: For training on TPUs.\n",
    "\n",
    "**Choosing a Strategy**:\n",
    "- Consider hardware: If you have multiple GPUs on a single machine, MirroredStrategy might be more suitable.\n",
    "- Consider scaling requirements: If you need to scale across multiple servers, MultiWorkerMirroredStrategy is more appropriate.\n",
    "- Consider the complexity of setting up the environment: TPUs require different setup and optimizations.\n",
    "\n",
    "The choice will depend on the specific requirements, hardware availability, and the scale of your application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad387f06",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "07c1a135",
   "metadata": {},
   "source": [
    "### 1. Write the Python code to implement a single neuron."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "58dbde1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def single_neuron(X, weights, bias):\n",
    "    return np.dot(X, weights) + bias\n",
    "\n",
    "X = np.array([0.4, 0.5, 0.6])\n",
    "weights = np.array([0.1, 0.2, 0.3])\n",
    "bias = 0.4\n",
    "\n",
    "output = single_neuron(X, weights, bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "83fe0c5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.72"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c07a41a",
   "metadata": {},
   "source": [
    "\n",
    "### 2. Write the Python code to implement ReLU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "80f434e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "def relu(x):\n",
    "    return max(0, x)\n",
    "\n",
    "# Example usage\n",
    "print(relu(-5))  # Output will be 0\n",
    "print(relu(5))   # Output will be 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fdce890",
   "metadata": {},
   "source": [
    "### 3. Write the Python code for a dense layer in terms of matrix multiplication."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6dca17be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dense_layer(X, weights, bias):\n",
    "    return np.dot(X, weights) + bias\n",
    "\n",
    "X = np.array([[0.4, 0.5, 0.6],\n",
    "              [0.1, 0.2, 0.3]])\n",
    "weights = np.array([[0.1, 0.2],\n",
    "                    [0.3, 0.4],\n",
    "                    [0.5, 0.6]])\n",
    "bias = np.array([0.1, 0.2])\n",
    "\n",
    "output = dense_layer(X, weights, bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "be2ddd29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.59, 0.84],\n",
       "       [0.32, 0.48]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49a1fc94",
   "metadata": {},
   "source": [
    "### 4. Write the Python code for a dense layer in plain Python (that is, with list comprehensions and functionality built into Python)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fe96218c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dense_layer_plain_python(X, weights, bias):\n",
    "    return [\n",
    "        [\n",
    "            sum(x * w + b for x, w, b in zip(X_row, W_col, bias))\n",
    "            for W_col in zip(*weights)\n",
    "        ]\n",
    "        for X_row in X\n",
    "    ]\n",
    "\n",
    "X = [[0.4, 0.5, 0.6],\n",
    "     [0.1, 0.2, 0.3]]\n",
    "weights = [[0.1, 0.2],\n",
    "           [0.3, 0.4],\n",
    "           [0.5, 0.6]]\n",
    "bias = [0.1, 0.2]\n",
    "\n",
    "output = dense_layer_plain_python(X, weights, bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3566854f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0.49, 0.5800000000000001], [0.37, 0.4]]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10bbef96",
   "metadata": {},
   "source": [
    "### 5. What is the “hidden size” of a layer?\n",
    "\n",
    "The \"hidden size\" of a layer refers to the number of neurons or units in that layer.\n",
    "\n",
    "---\n",
    "\n",
    "### 6. What does the `t` method do in PyTorch?\n",
    "\n",
    "The `t` method transposes a tensor in PyTorch. For 2D tensors, it swaps rows with columns.\n",
    "\n",
    "---\n",
    "\n",
    "### 7. Why is matrix multiplication written in plain Python very slow?\n",
    "\n",
    "Matrix multiplication in plain Python is slow due to the nested loops and the fact that Python is an interpreted language, which makes operations like this inefficient compared to optimized libraries like NumPy.\n",
    "\n",
    "---\n",
    "\n",
    "### 8. In `matmul`, why is `ac==br`?\n",
    "\n",
    "In matrix multiplication, the number of columns in the first matrix (`a`) must be equal to the number of rows in the second matrix (`b`) for the multiplication to be defined.\n",
    "\n",
    "---\n",
    "\n",
    "### 9. In Jupyter Notebook, how do you measure the time taken for a single cell to execute?\n",
    "\n",
    "You can use the `%%time` magic command at the start of the cell.\n",
    "\n",
    "---\n",
    "\n",
    "### 10. What is elementwise arithmetic?\n",
    "\n",
    "Elementwise arithmetic performs operations on corresponding elements between two arrays.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8288ed04",
   "metadata": {},
   "source": [
    "### 11. Write the PyTorch code to test whether every element of `a` is greater than the corresponding element of `b`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1445426c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "a = torch.tensor([1, 2, 3])\n",
    "b = torch.tensor([0, 1, 4])\n",
    "\n",
    "result = torch.gt(a, b)  # or a > b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2f50c3f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ True,  True, False])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ce40b8a",
   "metadata": {},
   "source": [
    "### 12. What is a rank-0 tensor? How do you convert it to a plain Python data type?\n",
    "\n",
    "A rank-0 tensor is a tensor containing a single scalar value. You can convert it to a plain Python data type using `.item()` in PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "606862be",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor(42)\n",
    "plain_python_x = x.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8ea66524",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plain_python_x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cdb9c46",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 13. How does elementwise arithmetic help us speed up `matmul`?\n",
    "\n",
    "Elementwise operations are more efficient because they can be parallelized easily, allowing for hardware-level optimizations.\n",
    "\n",
    "---\n",
    "\n",
    "### 14. What are the broadcasting rules?\n",
    "\n",
    "Broadcasting allows tensors with different shapes to be combined. The smaller tensor is \"broadcast\" across the larger tensor so that they have compatible shapes.\n",
    "\n",
    "1. Dimensions are compared element-wise starting from the trailing dimensions.\n",
    "2. Two dimensions are compatible if they are equal or if one of them is 1.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f7b72ad",
   "metadata": {},
   "source": [
    "### 15. What is `expand_as`? Show an example of how it can be used to match the results of broadcasting.\n",
    "\n",
    "`expand_as` takes a tensor and expands it to the size of another tensor. This is useful for manual broadcasting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5e092a61",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "a = torch.tensor([[1, 2]])\n",
    "b = torch.tensor([[1, 2], [3, 4]])\n",
    "\n",
    "expanded_a = a.expand_as(b)\n",
    "\n",
    "# expanded_a will be [[1, 2], [1, 2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1019f7ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2],\n",
       "        [1, 2]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "expanded_a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54b2bbd6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

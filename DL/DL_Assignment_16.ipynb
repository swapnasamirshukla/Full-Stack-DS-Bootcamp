{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cc497f1d",
   "metadata": {},
   "source": [
    "### Question: Explain the Activation Functions in your own language.\n",
    "\n",
    "**Answer:**\n",
    "\n",
    "Activation functions are like traffic lights for neural networks. They decide how much signal should pass through, affecting the network's output.\n",
    "\n",
    "1. **Sigmoid**: It's like a cautious driver who slows down as they approach red and green lights. The function squashes values between 0 and 1, making sure things don't go extreme.\n",
    "   \n",
    "2. **Tanh**: A more balanced driver than sigmoid. Still cautious, but handles both positive and negative signals well, squashing values between -1 and 1.\n",
    "\n",
    "3. **ReLU (Rectified Linear Unit)**: This one is like a free-wheeling biker who ignores anything negative and speeds up with positive signals. It turns all negative values to zero and lets positive values pass as is.\n",
    "\n",
    "4. **ELU (Exponential Linear Unit)**: A considerate driver that slows down but doesnâ€™t completely stop for negative signals. It can go a bit below zero for negative inputs, helping the network learn faster.\n",
    "\n",
    "5. **LeakyReLU**: A slightly impatient driver who doesn't want to stop for small negative signals. Instead of setting them to zero, it scales them down but lets them pass.\n",
    "\n",
    "6. **Swish**: A sophisticated driver who adjusts speed dynamically, sometimes slowing down, sometimes speeding up, optimizing the ride. It's a more flexible function that often performs well in practice.\n",
    "\n",
    "---\n",
    "\n",
    "### Question: What happens when you increase or decrease the optimizer learning rate?\n",
    "\n",
    "**Answer:**\n",
    "\n",
    "Think of the learning rate like the speed of a car. Too fast and you might overshoot your destination (overshoot optimal values); too slow and you might never get there (converge too slowly). Increasing the learning rate can make your network learn faster but risk overshooting, while decreasing it makes the network more cautious but could be too slow.\n",
    "\n",
    "---\n",
    "\n",
    "### Question: What happens when you increase the number of internal hidden neurons?\n",
    "\n",
    "**Answer:**\n",
    "\n",
    "Adding more hidden neurons is like hiring more employees for a task. You'll potentially solve more complex problems but risk overcomplicating simple tasks, making your model harder to manage and possibly overfitting the data.\n",
    "\n",
    "---\n",
    "\n",
    "### Question: What happens when you increase the size of batch computation?\n",
    "\n",
    "**Answer:**\n",
    "\n",
    "Increasing batch size is like bulk buying in a store. You get more items processed at once, making it more efficient but requiring more computational resources. However, your optimization might become less precise as you're making updates based on a larger set of data at once.\n",
    "\n",
    "---\n",
    "\n",
    "### Question: Why we adopt regularization to avoid overfitting?\n",
    "\n",
    "**Answer:**\n",
    "\n",
    "Regularization is like a fitness coach that prevents you from overtraining on one specific exercise. It adds a penalty term to ensure that the network doesn't get too specialized on the training data, allowing it to generalize better to new data.\n",
    "\n",
    "---\n",
    "\n",
    "### Question: What are loss and cost functions in deep learning?\n",
    "\n",
    "**Answer:**\n",
    "\n",
    "Loss and cost functions are like the scoreboards in a game. They tell you how far off your predictions are from the actual result. Loss function measures the error for a single data point, while the cost function is the average loss over the entire dataset.\n",
    "\n",
    "---\n",
    "\n",
    "### Question: What do you mean by underfitting in neural networks?\n",
    "\n",
    "**Answer:**\n",
    "\n",
    "Underfitting is like undercooking a dish. Your model hasn't learned enough from the data, so it performs poorly even on the training data, let alone new data. It means your network is too simplistic to capture the complexities of the data.\n",
    "\n",
    "---\n",
    "\n",
    "### Question: Why we use Dropout in Neural Networks?\n",
    "\n",
    "**Answer:**\n",
    "\n",
    "Dropout is like a team-building exercise where you randomly sit out some players during practice. This forces the team to adapt and learn to work with different configurations, making the neural network more robust and preventing any single neuron from becoming too specialized.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94072860",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

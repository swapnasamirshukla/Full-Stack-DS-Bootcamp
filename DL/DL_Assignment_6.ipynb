{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "864f66b1",
   "metadata": {},
   "source": [
    "### 1. What are the advantages of a CNN over a fully connected DNN for image classification?\n",
    "\n",
    "CNNs have several advantages over fully connected DNNs for image classification:\n",
    "\n",
    "- Parameter Efficiency: CNNs generally use fewer parameters, reducing the computational cost and the risk of overfitting.\n",
    "  \n",
    "- Feature Hierarchies: CNNs can learn hierarchical features, from edges to complex structures, which is beneficial for image data.\n",
    "\n",
    "- Translational Invariance: CNNs are better equipped to handle translational variance in images.\n",
    "\n",
    "- Local Connectivity: CNNs consider the local spatial coherence of pixels, making them more semantically meaningful for image tasks.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Consider a CNN composed of three convolutional layers, each with 3 Ã— 3 kernels, a stride of 2, and \"same\" padding. What is the total number of parameters?\n",
    "\n",
    "The total number of parameters in this CNN would be calculated as follows:\n",
    "\n",
    "- First Conv Layer: Parameters = \\( (3 \\times 3 \\times 3 + 1) \\times 100 = 2800 \\) (Weights + Bias)\n",
    "- Second Conv Layer: Parameters = \\( (3 \\times 3 \\times 100 + 1) \\times 200 = 180200 \\)\n",
    "- Third Conv Layer: Parameters = \\( (3 \\times 3 \\times 200 + 1) \\times 400 = 720400 \\)\n",
    "\n",
    "Total Parameters = 2800 + 180200 + 720400 = 903400\n",
    "\n",
    "---\n",
    "\n",
    "### 3. If your GPU runs out of memory while training a CNN, what are five things you could try to solve the problem?\n",
    "\n",
    "If you're running out of GPU memory, you could try:\n",
    "\n",
    "1. Decrease the Batch Size: This will lower the memory requirement for each training iteration.\n",
    "\n",
    "2. Gradient Accumulation: Use smaller batches but accumulate gradients over multiple steps before performing an update.\n",
    "\n",
    "3. Simplify the Model: Reduce the complexity of your CNN by lowering the number of layers or units in each layer.\n",
    "\n",
    "4. Data Generators: Stream data into the model batch-by-batch instead of loading it all into memory at once.\n",
    "\n",
    "5. Mixed-Precision Training: Use a combination of 16-bit and 32-bit floating-point numbers to reduce memory usage.\n",
    "\n",
    "---\n",
    "\n",
    "### 4. Why would you want to add a max pooling layer rather than a convolutional layer with the same stride?\n",
    "\n",
    "Max pooling layers offer advantages such as:\n",
    "\n",
    "- Parameter Efficiency: Max pooling has zero parameters, thereby not increasing the model complexity.\n",
    "  \n",
    "- Effective Downsampling: It provides an aggressive way to reduce spatial dimensions, which can help in focusing on more abstract features.\n",
    "\n",
    "---\n",
    "\n",
    "### 5. When would you want to add a local response normalization layer?\n",
    "\n",
    "Local response normalization is generally used when:\n",
    "\n",
    "- You want to encourage competition between adjacent feature maps.\n",
    "  \n",
    "- You expect neurons detecting similar features to be highly activated and aim to dampen the responses that are uniformly large across all feature maps.\n",
    "\n",
    "---\n",
    "\n",
    "### 6. Can you name the main innovations in AlexNet, compared to LeNet-5? What about the main innovations in GoogLeNet, ResNet, SENet, and Xception?\n",
    "\n",
    "- AlexNet: Brought deeper architectures, ReLU activation, and dropout.\n",
    "  \n",
    "- GoogLeNet: Introduced the inception modules to capture multi-scale information.\n",
    "\n",
    "- ResNet: Utilized residual connections to train deeper networks effectively.\n",
    "\n",
    "- SENet: Introduced Squeeze-and-Excitation blocks to recalibrate feature maps.\n",
    "\n",
    "- Xception: Employed depthwise separable convolutions for more efficient computation.\n",
    "\n",
    "---\n",
    "\n",
    "### 7. What is a fully convolutional network? How can you convert a dense layer into a convolutional layer?\n",
    "\n",
    "A fully convolutional network is a CNN where all layers are convolutional. To convert a dense layer to a convolutional layer, you can replace it with a convolutional layer whose kernel size matches the spatial dimensions of the input volume.\n",
    "\n",
    "---\n",
    "\n",
    "### 8. What is the main technical difficulty of semantic segmentation?\n",
    "\n",
    "The main challenge in semantic segmentation is to maintain spatial resolution throughout the network so that the output segmentation map is precise. This is difficult because typical CNN architectures downsample the input to extract features.\n",
    "\n",
    "---\n",
    "\n",
    "### 9. Build your own CNN from scratch and try to achieve the highest possible accuracy on MNIST.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "046558ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "750/750 [==============================] - 14s 18ms/step - loss: 0.2177 - accuracy: 0.9319 - val_loss: 0.0740 - val_accuracy: 0.9787\n",
      "Epoch 2/10\n",
      "750/750 [==============================] - 13s 18ms/step - loss: 0.0551 - accuracy: 0.9826 - val_loss: 0.0602 - val_accuracy: 0.9822\n",
      "Epoch 3/10\n",
      "750/750 [==============================] - 13s 17ms/step - loss: 0.0375 - accuracy: 0.9876 - val_loss: 0.0385 - val_accuracy: 0.9891\n",
      "Epoch 4/10\n",
      "750/750 [==============================] - 13s 17ms/step - loss: 0.0288 - accuracy: 0.9908 - val_loss: 0.0371 - val_accuracy: 0.9889\n",
      "Epoch 5/10\n",
      "750/750 [==============================] - 13s 17ms/step - loss: 0.0244 - accuracy: 0.9921 - val_loss: 0.0402 - val_accuracy: 0.9878\n",
      "Epoch 6/10\n",
      "750/750 [==============================] - 13s 17ms/step - loss: 0.0189 - accuracy: 0.9937 - val_loss: 0.0372 - val_accuracy: 0.9893\n",
      "Epoch 7/10\n",
      "750/750 [==============================] - 13s 18ms/step - loss: 0.0163 - accuracy: 0.9944 - val_loss: 0.0350 - val_accuracy: 0.9906\n",
      "Epoch 8/10\n",
      "750/750 [==============================] - 14s 18ms/step - loss: 0.0130 - accuracy: 0.9958 - val_loss: 0.0379 - val_accuracy: 0.9894\n",
      "Epoch 9/10\n",
      "750/750 [==============================] - 14s 19ms/step - loss: 0.0100 - accuracy: 0.9966 - val_loss: 0.0507 - val_accuracy: 0.9874\n",
      "Epoch 10/10\n",
      "750/750 [==============================] - 14s 18ms/step - loss: 0.0110 - accuracy: 0.9961 - val_loss: 0.0433 - val_accuracy: 0.9900\n",
      "313/313 [==============================] - 1s 4ms/step - loss: 0.0299 - accuracy: 0.9920\n",
      "Test accuracy: 99.20%\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Load the MNIST dataset\n",
    "mnist = tf.keras.datasets.mnist\n",
    "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
    "\n",
    "# Preprocess the data\n",
    "train_images = train_images.reshape((60000, 28, 28, 1)).astype('float32') / 255\n",
    "test_images = test_images.reshape((10000, 28, 28, 1)).astype('float32') / 255\n",
    "train_labels = tf.keras.utils.to_categorical(train_labels, 10)\n",
    "test_labels = tf.keras.utils.to_categorical(test_labels, 10)\n",
    "\n",
    "# Build the CNN model\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)),\n",
    "    tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "    tf.keras.layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "    tf.keras.layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(train_images, train_labels, epochs=10, batch_size=64, validation_split=0.2)\n",
    "\n",
    "# Evaluate the model\n",
    "test_loss, test_acc = model.evaluate(test_images, test_labels)\n",
    "print(\"Test accuracy: {:.2f}%\".format(test_acc * 100))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "677161e2",
   "metadata": {},
   "source": [
    "### 10. Use transfer learning for large image classification.\n",
    "\n",
    "a. Create a training set containing at least 100 images per class. For example, you could classify your own pictures based on the location (beach, mountain, city, etc.), or alternatively you can use an existing dataset (e.g., from TensorFlow Datasets).\n",
    "\n",
    "b. Split it into a training set, a validation set, and a test set.\n",
    "\n",
    "c. Build the input pipeline, including the appropriate preprocessing operations, and optionally add data augmentation.\n",
    "\n",
    "d. Fine-tune a pretrained model on this dataset.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "165760ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mDownloading and preparing dataset Unknown size (download: Unknown size, generated: Unknown size, total: Unknown size) to C:\\Users\\shuklas\\tensorflow_datasets\\tf_flowers\\3.0.1...\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a71d2f411db8493b91666f9044643240",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dl Completed...: 0 url [00:00, ? url/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fde7443fa8774c0b993f430d2ecab253",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dl Size...: 0 MiB [00:00, ? MiB/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating splits...:   0%|          | 0/1 [00:00<?, ? splits/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train examples...: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Shuffling C:\\Users\\shuklas\\tensorflow_datasets\\tf_flowers\\3.0.1.incompleteVINU9M\\tf_flowers-train.tfrecord*...â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mDataset tf_flowers downloaded and prepared to C:\\Users\\shuklas\\tensorflow_datasets\\tf_flowers\\3.0.1. Subsequent calls will reuse this data.\u001b[0m\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "58889256/58889256 [==============================] - 5s 0us/step\n",
      "Epoch 1/10\n",
      "92/92 [==============================] - 404s 4s/step - loss: 1.4585 - accuracy: 0.4346 - val_loss: 1.2761 - val_accuracy: 0.5872\n",
      "Epoch 2/10\n",
      "92/92 [==============================] - 454s 5s/step - loss: 1.1684 - accuracy: 0.6393 - val_loss: 1.0900 - val_accuracy: 0.6471\n",
      "Epoch 3/10\n",
      "92/92 [==============================] - 452s 5s/step - loss: 1.0137 - accuracy: 0.6860 - val_loss: 0.9603 - val_accuracy: 0.6785\n",
      "Epoch 4/10\n",
      "92/92 [==============================] - 449s 5s/step - loss: 0.9188 - accuracy: 0.7067 - val_loss: 0.8860 - val_accuracy: 0.6989\n",
      "Epoch 5/10\n",
      "92/92 [==============================] - 435s 5s/step - loss: 0.8500 - accuracy: 0.7343 - val_loss: 0.8248 - val_accuracy: 0.7398\n",
      "Epoch 6/10\n",
      "92/92 [==============================] - 423s 5s/step - loss: 0.8002 - accuracy: 0.7514 - val_loss: 0.7840 - val_accuracy: 0.7384\n",
      "Epoch 7/10\n",
      "92/92 [==============================] - 431s 5s/step - loss: 0.7610 - accuracy: 0.7602 - val_loss: 0.7451 - val_accuracy: 0.7684\n",
      "Epoch 8/10\n",
      "92/92 [==============================] - 427s 5s/step - loss: 0.7290 - accuracy: 0.7677 - val_loss: 0.7204 - val_accuracy: 0.7698\n",
      "Epoch 9/10\n",
      "92/92 [==============================] - 421s 5s/step - loss: 0.7029 - accuracy: 0.7745 - val_loss: 0.6980 - val_accuracy: 0.7738\n",
      "Epoch 10/10\n",
      "92/92 [==============================] - 414s 5s/step - loss: 0.6783 - accuracy: 0.7875 - val_loss: 0.6828 - val_accuracy: 0.7807\n",
      "Epoch 1/10\n",
      "92/92 [==============================] - 399s 4s/step - loss: 0.5194 - accuracy: 0.8188 - val_loss: 0.4511 - val_accuracy: 0.8420\n",
      "Epoch 2/10\n",
      "92/92 [==============================] - 348s 4s/step - loss: 0.3809 - accuracy: 0.8672 - val_loss: 0.4030 - val_accuracy: 0.8501\n",
      "Epoch 3/10\n",
      "92/92 [==============================] - 338s 4s/step - loss: 0.2991 - accuracy: 0.8982 - val_loss: 0.3514 - val_accuracy: 0.8678\n",
      "Epoch 4/10\n",
      "92/92 [==============================] - 331s 4s/step - loss: 0.2413 - accuracy: 0.9217 - val_loss: 0.3301 - val_accuracy: 0.8828\n",
      "Epoch 5/10\n",
      "92/92 [==============================] - 321s 3s/step - loss: 0.2026 - accuracy: 0.9336 - val_loss: 0.2938 - val_accuracy: 0.8910\n",
      "Epoch 6/10\n",
      "92/92 [==============================] - 335s 4s/step - loss: 0.1625 - accuracy: 0.9527 - val_loss: 0.2965 - val_accuracy: 0.8937\n",
      "Epoch 7/10\n",
      "92/92 [==============================] - 324s 4s/step - loss: 0.1318 - accuracy: 0.9629 - val_loss: 0.3118 - val_accuracy: 0.8924\n",
      "Epoch 8/10\n",
      "92/92 [==============================] - 360s 4s/step - loss: 0.1109 - accuracy: 0.9683 - val_loss: 0.2900 - val_accuracy: 0.8978\n",
      "Epoch 9/10\n",
      "92/92 [==============================] - 344s 4s/step - loss: 0.0829 - accuracy: 0.9802 - val_loss: 0.2863 - val_accuracy: 0.9033\n",
      "Epoch 10/10\n",
      "92/92 [==============================] - 410s 4s/step - loss: 0.0657 - accuracy: 0.9874 - val_loss: 0.3282 - val_accuracy: 0.8896\n",
      "23/23 [==============================] - 60s 3s/step - loss: 0.3282 - accuracy: 0.8896\n",
      "Final test accuracy: 88.96%\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "# Load the dataset (replace this with your dataset if needed)\n",
    "(train_set, test_set), dataset_info = tfds.load(\n",
    "    'tf_flowers',\n",
    "    split=['train[:80%]', 'train[80%:]'],\n",
    "    as_supervised=True,\n",
    "    with_info=True\n",
    ")\n",
    "\n",
    "# Preprocess the dataset\n",
    "def preprocess(dataset):\n",
    "    def _preprocess_img(image, label):\n",
    "        image = tf.image.resize(image, (224, 224))\n",
    "        image = image / 255.0  # normalize to [0,1]\n",
    "        return image, label\n",
    "    return dataset.map(_preprocess_img)\n",
    "\n",
    "train_set = preprocess(train_set).batch(32).shuffle(buffer_size=1000).prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
    "test_set = preprocess(test_set).batch(32)\n",
    "\n",
    "# Load a pre-trained model (VGG16)\n",
    "base_model = tf.keras.applications.VGG16(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "\n",
    "# Freeze the base model layers\n",
    "base_model.trainable = False\n",
    "\n",
    "# Create the final model\n",
    "model = tf.keras.Sequential([\n",
    "    base_model,\n",
    "    tf.keras.layers.GlobalAveragePooling2D(),\n",
    "    tf.keras.layers.Dense(dataset_info.features['label'].num_classes, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(),\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(train_set, epochs=10, validation_data=test_set)\n",
    "\n",
    "# Unfreeze some layers of the base model for fine-tuning\n",
    "base_model.trainable = True\n",
    "for layer in base_model.layers[:15]:\n",
    "    layer.trainable = False\n",
    "\n",
    "# Recompile the model (with a lower learning rate)\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(1e-5),\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Fine-tune the model\n",
    "model.fit(train_set, epochs=10, validation_data=test_set)\n",
    "\n",
    "# Evaluate the model\n",
    "loss, accuracy = model.evaluate(test_set)\n",
    "print(f\"Final test accuracy: {accuracy*100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "713cb964",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ad8e75e9",
   "metadata": {},
   "source": [
    "### 1. How does `unsqueeze` help us to solve certain broadcasting problems?\n",
    "\n",
    "The `unsqueeze` operation adds a singleton dimension to a tensor, which can help in aligning the dimensions of tensors for broadcasting. By introducing an extra axis with size 1, you make it possible for PyTorch or NumPy to successfully broadcast to the desired shape.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. How can we use indexing to do the same operation as `unsqueeze`?\n",
    "\n",
    "You can add a `None` index to introduce a new axis into the tensor. For example, if `a` is a 1D tensor of shape `(n,)`, then `a[:, None]` would introduce a new axis, making the shape `(n, 1)`.\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "a = np.array([1, 2, 3])\n",
    "b = a[:, None]\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 3. How do we show the actual contents of the memory used for a tensor?\n",
    "\n",
    "You can use the `.numpy()` method in PyTorch to convert a tensor to a NumPy array and inspect the underlying data. In NumPy, you can simply print the array.\n",
    "\n",
    "```python\n",
    "import torch\n",
    "\n",
    "a = torch.tensor([1, 2, 3])\n",
    "print(a.numpy())\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 4. When adding a vector of size 3 to a matrix of size 3Ã—3, are the elements of the vector added to each row or each column of the matrix?\n",
    "\n",
    "The elements of the vector are added to each row of the matrix.\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "\n",
    "vector = np.array([1, 2, 3])\n",
    "matrix = np.array([[4, 5, 6],\n",
    "                   [7, 8, 9],\n",
    "                   [10, 11, 12]])\n",
    "\n",
    "result = matrix + vector  # Broadcasting happens along rows\n",
    "print(result)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 5. Do broadcasting and `expand_as` result in increased memory use? Why or why not?\n",
    "\n",
    "No, they don't usually result in increased memory usage. Broadcasting performs the arithmetic operation without actually duplicating the data in memory. `expand_as` in PyTorch also returns a new tensor that has the same data as its input tensor but with a different size, without actually allocating new memory.\n",
    "\n",
    "---\n",
    "\n",
    "### 6. Implement `matmul` using Einstein summation.\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "\n",
    "def matmul_einsum(A, B):\n",
    "    return np.einsum('ij,jk->ik', A, B)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 7. What does a repeated index letter represent on the lefthand side of `einsum`?\n",
    "\n",
    "In Einstein summation notation, a repeated index letter on the lefthand side represents a contraction over that index. This effectively means performing a sum over that index.\n",
    "\n",
    "---\n",
    "\n",
    "### 8. What are the three rules of Einstein summation notation? Why?\n",
    "\n",
    "1. Each index can appear at most twice in any term.\n",
    "2. Each term must have the same indices.\n",
    "3. Any index appearing exactly once in each term is a \"free index\" and represents a dimension in the output. An index appearing twice is a \"dummy index\" and is summed over.\n",
    "\n",
    "These rules provide a concise way to express complex array manipulations and contractions.\n",
    "\n",
    "---\n",
    "\n",
    "### 9. What are the forward pass and backward pass of a neural network?\n",
    "\n",
    "- Forward Pass: Computation flows from input to output, calculating the prediction of the neural network for a given input.\n",
    "  \n",
    "- Backward Pass: The process of backpropagation computes the gradient of the loss function with respect to each parameter by applying the chain rule of calculus.\n",
    "\n",
    "---\n",
    "\n",
    "### 10. Why do we need to store some of the activations calculated for intermediate layers in the forward pass?\n",
    "\n",
    "These stored activations are needed during the backward pass to compute gradients. Without them, you would have to recompute them, which would be inefficient.\n",
    "\n",
    "---\n",
    "\n",
    "### 11. What is the downside of having activations with a standard deviation too far away from 1?\n",
    "\n",
    "Having a standard deviation too far from 1 can lead to issues like vanishing or exploding gradients, which in turn can make the neural network difficult to train effectively.\n",
    "\n",
    "---\n",
    "\n",
    "### 12. How can weight initialization help avoid this problem?\n",
    "\n",
    "Proper weight initialization helps in maintaining the variance of activations and gradients as they pass through layers, thereby mitigating issues like vanishing or exploding gradients. Methods like Xavier (Glorot) initialization or He initialization are commonly used for this purpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76088ddc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

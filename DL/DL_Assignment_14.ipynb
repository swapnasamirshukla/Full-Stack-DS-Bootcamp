{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7b9737ff",
   "metadata": {},
   "source": [
    "### Question: Is it okay to initialize all the weights to the same value as long as that value is selected randomly using He initialization?\n",
    "\n",
    "**Answer:**\n",
    "\n",
    "No, it's not a good idea to initialize all the weights to the same value, even if that value is derived from a method like He initialization. The issue is that if all weights have the same initial value, then all neurons in a given layer of the network will produce the same output during the forward pass and receive the same gradient during the backpropagation. This will make it hard for the network to learn diverse features, essentially making many neurons redundant.\n",
    "\n",
    "---\n",
    "\n",
    "### Question: Is it okay to initialize the bias terms to 0?\n",
    "\n",
    "**Answer:**\n",
    "\n",
    "Yes, it is generally okay to initialize bias terms to 0. Unlike weights, initializing biases to zero usually doesn't lead to symmetry issues during training. The gradients during backpropagation will update the biases in a way that helps the model learn.\n",
    "\n",
    "---\n",
    "\n",
    "### Question: Name three advantages of the ELU activation function over ReLU.\n",
    "\n",
    "**Answer:**\n",
    "\n",
    "1. **Handling Negative Inputs**: ELU allows small negative outputs for negative inputs, which helps the network to learn faster by pushing the mean activations towards zero.\n",
    "\n",
    "2. **Reduced Dead Neurons**: In ReLU, neurons can sometimes get \"stuck\" during training if they receive only negative inputs. ELU avoids this problem.\n",
    "\n",
    "3. **Smoothness**: ELU is a smooth function, allowing for smoother gradients and potentially better optimization.\n",
    "\n",
    "---\n",
    "\n",
    "### Question: In which cases would you want to use each of the following activation functions: ELU, leaky ReLU (and its variants), ReLU, tanh, logistic, and softmax?\n",
    "\n",
    "**Answer:**\n",
    "\n",
    "1. **ELU**: Useful in deeper networks to speed up learning and reduce the \"dead neuron\" problem. \n",
    "2. **Leaky ReLU and its variants**: Good for scenarios where you're concerned about dead neurons but want faster computation than ELU.\n",
    "3. **ReLU**: A go-to for many applications due to its computational efficiency but can have issues with dead neurons.\n",
    "4. **Tanh**: Good for binary classification problems where you want outputs centered around zero.\n",
    "5. **Logistic (Sigmoid)**: Mostly used in the output layer for binary classification problems.\n",
    "6. **Softmax**: Generally used in the output layer for multi-class classification problems.\n",
    "\n",
    "---\n",
    "\n",
    "### Question: What may happen if you set the momentum hyperparameter too close to 1 (e.g., 0.99999) when using a MomentumOptimizer?\n",
    "\n",
    "**Answer:**\n",
    "\n",
    "Setting the momentum hyperparameter too close to 1 might make the optimizer overshoot the optimal values frequently and oscillate around the minimum, making it harder for the model to converge. The model could also become very sensitive to local fluctuations in the loss landscape.\n",
    "\n",
    "---\n",
    "\n",
    "### Question: Name three ways you can produce a sparse model.\n",
    "\n",
    "**Answer:**\n",
    "\n",
    "1. **L1 Regularization**: Using L1 regularization during training can push many weights towards zero, making the model sparse.\n",
    "  \n",
    "2. **Pruning**: After training a dense model, you can remove the neurons with the smallest weights.\n",
    "\n",
    "3. **Feature Selection**: You can pre-select only the most important features before training, effectively creating a sparse model.\n",
    "\n",
    "---\n",
    "\n",
    "### Question: Does dropout slow down training? Does it slow down inference (i.e., making predictions on new instances)?\n",
    "\n",
    "**Answer:**\n",
    "\n",
    "1. **Training**: Dropout can slow down training to some extent because it introduces additional randomness and requires the model to learn from a \"thinned\" version of itself at each iteration.\n",
    "\n",
    "2. **Inference**: Dropout is usually turned off during inference, so it does not slow down the process of making predictions on new instances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f82fd043",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
